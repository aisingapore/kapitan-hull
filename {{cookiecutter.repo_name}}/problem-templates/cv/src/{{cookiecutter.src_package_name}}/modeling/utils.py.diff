--- {{cookiecutter.repo_name}}/src/{{cookiecutter.src_package_name}}/modeling/utils.py
+++ {{cookiecutter.repo_name}}/problem-templates/cv/src/{{cookiecutter.src_package_name}}/modeling/utils.py
@@ -1,132 +1,168 @@
 """Utilities for model training and experimentation workflows.
 """
-import time
-import random
-import logging
+import torch
 
 import {{cookiecutter.src_package_name}} as {{cookiecutter.src_package_name_short}}
 
-logger = logging.getLogger(__name__)
 
-
-def train(mlflow_init_status, model, dataset, epoch, learning_rate=0.01, batch_size=32):
-    """Simulate training a model.
+def train(args, model, device, train_loader, optimiser, epoch, mlflow_init_status):
+    """Trains model.
 
     Parameters
     ----------
+    args : omegaconf.DictConfig
+        An omegaconf.DictConfig object containing arguments from the main function.
+    model : `torch.nn.Module` object
+        Initialised PyTorch model.
+    device : `torch.device` object
+        Device type to be used for training.
+    train_loader : `torch.utils.data.DataLoader` object
+        Iterable that provides batches of data from the training dataset.
+    optimiser : `torch.optim.Optimizer` object
+        Optimiser algorithm to be used for training.
+    epoch : int
+        Current epoch value.
     mlflow_init_status : bool
         Boolean value indicative of success of intialising connection
         with MLflow server.
-    model : object
-        The model to train.
-    dataset : object
-        The dataset.
-    epoch : int
-        Current epoch value.
-    learning_rate : float, optional
-        Learning rate for training, by default 0.01.
-    batch_size : int, optional
-        Batch size for training, by default 32.
 
     Returns
     -------
-    float
-        Simulated loss value for the training dataset.
+    loss.item() : float
+        Average loss value for the training dataset.
     """
-    
     model.train()
-    
-    # Generate fake metrics that improve over time
-    loss = 1.0 - (0.15 * epoch) + random.uniform(-0.05, 0.05)
-    loss = max(0.1, loss)  # Ensure loss doesn't go below 0.1
-    accuracy = 0.5 + (0.08 * epoch) + random.uniform(-0.02, 0.02)
-    accuracy = min(0.98, accuracy)  # Cap accuracy at 0.98
-    
-    logger.info(f"Epoch {epoch} - loss: {loss:.4f}, accuracy: {accuracy:.4f}")
-    
+    correct = 0
+    dataset_len = len(train_loader.dataset)
+    for batch_idx, (_, data, target) in enumerate(train_loader):
+        data, target = data.to(device), target.to(device)
+        optimiser.zero_grad()
+        output = model(data)
+        loss = torch.nn.functional.nll_loss(output, target)
+        loss.backward()
+        optimiser.step()
+        pred = output.argmax(dim=1, keepdim=True)
+        correct += pred.eq(target.view_as(pred)).sum().item()
+        if (batch_idx + 1) % args["log_interval"] == 0:
+            curr_progress = (batch_idx + 1) * len(data)
+            print(
+                "Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tAccuracy: {:.4f}".format(
+                    epoch,
+                    curr_progress,
+                    dataset_len,
+                    100.0 * curr_progress / dataset_len,
+                    loss.item(),
+                    correct / curr_progress
+                )
+            )
+            if args["dry_run"]:
+                break
+
     {{cookiecutter.src_package_name_short}}.general_utils.mlflow_log(
         mlflow_init_status,
         "log_metric",
         key="train_loss",
-        value=loss,
+        value=loss.item(),
         step=epoch,
     )
-
     {{cookiecutter.src_package_name_short}}.general_utils.mlflow_log(
         mlflow_init_status,
         "log_metric",
         key="train_accuracy",
-        value=accuracy,
+        value=correct / dataset_len,
         step=epoch,
     )
 
-    return loss
+    return loss.item()
 
 
-def test(mlflow_init_status, model, dataset, epoch, test_size=100):
-    """Simulate testing a model.
+def test(model, device, test_loader, epoch, mlflow_init_status):
+    """Evaluate model on test dataset.
 
     Parameters
     ----------
+    model : `torch.nn.Module` object
+        Initialised PyTorch model.
+    device : `torch.device` object
+        Device type to be used for training.
+    test_loader : `torch.utils.data.DataLoader` object
+        Iterable that provides batches of data from the test dataset.
+    epoch : int
+        Current epoch value.
     mlflow_init_status : bool
         Boolean value indicative of success of intialising connection
         with MLflow server.
-    model : object
-        The model to test.
-    dataset : object
-        The dataset.
-    epoch : int
-        Current epoch value.
-    test_size : int, optional
-        Number of test samples, by default 100.
 
     Returns
     -------
     test_loss : float
         Average loss value for the test dataset.
     test_accuracy : float
         Accuracy value for the test dataset.
     """
-    logger.info(f"Evaluating model on {test_size} test samples")
-    
-    model.predict()
-    
-    # Generate fake test metrics
-    test_loss = random.uniform(0.1, 0.3)
-    test_accuracy = random.uniform(0.85, 0.95)
-    
-    logger.info(f"Test results - loss: {test_loss:.4f}, accuracy: {test_accuracy:.4f}")
-    
-    {{cookiecutter.src_package_name_short}}.general_utils.mlflow_log(
-        mlflow_init_status,
-        "log_metric",
-        key="test_loss",
-        value=test_loss,
-        step=epoch
+    model.eval()
+    test_loss = 0
+    correct = 0
+    with torch.no_grad():
+        for _, data, target in test_loader:
+            data, target = data.to(device), target.to(device)
+            output = model(data)
+            test_loss += torch.nn.functional.nll_loss(
+                output, target, reduction="sum"
+            ).item()
+            pred = output.argmax(dim=1, keepdim=True)
+            correct += pred.eq(target.view_as(pred)).sum().item()
+
+    test_loss /= len(test_loader.dataset)
+    test_accuracy = correct / len(test_loader.dataset)
+
+    print(
+        "\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n".format(
+            test_loss,
+            correct,
+            len(test_loader.dataset),
+            100 * test_accuracy,
+        )
     )
 
+    {{cookiecutter.src_package_name_short}}.general_utils.mlflow_log(
+        mlflow_init_status, "log_metric", key="test_loss", value=test_loss, step=epoch
+    )
     {{cookiecutter.src_package_name_short}}.general_utils.mlflow_log(
         mlflow_init_status,
         "log_metric",
         key="test_accuracy",
         value=test_accuracy,
-        step=epoch
+        step=epoch,
     )
 
     return test_loss, test_accuracy
 
 
-def load_model():
-    """Load dummy model.
+def load_model(path_to_model, use_cuda, use_mps):
+    """Load PyTorch model state dict.
 
-    A sample utility function to be used.
+    A sample utility function to be used for loading a PyTorch model.
+
+    Parameters
+    ----------
+    path_to_model : str
+        Path to a PyTorch checkpoint model.
 
     Returns
     -------
-    loaded_model : 
-        Object containing dummy model.
+    loaded_model : `torch.nn.state_dict` object
+        Object containing state of predictive model.
+    device : `torch.device` object
+        Device type to be used for whatever operation will be using this variable.
     """
-    loaded_model = {{cookiecutter.src_package_name_short}}.modeling.models.DummyModel()
-
-    return loaded_model
-
+    if use_cuda and torch.cuda.is_available():
+        device = torch.device("cuda")
+    elif use_mps and torch.backends.mps.is_available():
+        device = torch.device("mps")
+    else:
+        device = torch.device("cpu")
+    loaded_model = {{cookiecutter.src_package_name_short}}.modeling.models.Net().to(device)
+    checkpoint = torch.load(path_to_model)
+    loaded_model.load_state_dict(checkpoint["model_state_dict"])
+    return loaded_model, device
