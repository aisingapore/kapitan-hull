--- {{cookiecutter.repo_name}}/aisg-context/guide-site/docs/runai/06c-job-orchestration.md
+++ {{cookiecutter.repo_name}}/problem-templates/cv/aisg-context/guide-site/docs/runai/06c-job-orchestration.md
@@ -43,10 +43,10 @@
 Run:ai. You can first your configuration variables at 
 `conf/process_data.yaml`, specifically this section:
 
 ```yaml
-raw_data_dir_path: "./data/raw"
-processed_data_dir_path: "./data/processed"
+raw_data_dir_path: "./data/mnist-pngs-data-aisg"
+processed_data_dir_path: "./data/processed/mnist-pngs-data-aisg-processed"
 ```
 
 This requires the Docker image to be built from a Dockerfile 
 (`docker/{{cookiecutter.src_package_name}}-cpu.Dockerfile`)
@@ -90,9 +90,9 @@
 
 === "Coder Workspace Terminal using Run:ai"
 
     ```bash
-    # Run `runai login` and `runai config project {{cookiecutter.proj_name}}` first if needed
+    # Run `runai login` and `runai config project aisg` first if needed
     # Run this in the base of your project repository, and change accordingly
     # Switch working-dir to /<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}} to use the repo in the PVC
     runai submit \
         --job-name-prefix <YOUR_HYPHENATED_NAME>-data-prep \
@@ -100,16 +100,16 @@
         --working-dir /home/aisg/{{cookiecutter.repo_name}} \
         --existing-pvc claimname=<NAME_OF_DATA_SOURCE>,path=/<NAME_OF_DATA_SOURCE> \
         --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \
         --command -- /bin/bash -c "python -u src/process_data.py \
-            raw_data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/raw \
-            processed_data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed"
+            raw_data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/mnist-pngs-data-aisg \
+            processed_data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed/mnist-pngs-data-aisg-processed"
     ```
 
 After some time, the data processing job should conclude and we can
 proceed with training the predictive model.
 The processed data is exported to the directory
-`/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed`.
+`/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed/mnist-pngs-data-aisg-processed`.
 We will be passing this path to the model training workflows.
 
 ## Model Training
 
@@ -138,8 +138,12 @@
     namespace for projects that requires model experimentation. 
     Artifacts logged through the MLflow API can be uploaded to {{objstg}} 
     buckets, assuming the client is authorised for access to {{objstg}}.
 
+    !!! note
+        The username and password for the MLflow Tracking server
+        can be retrieved from the MLOps team or your team lead.
+
     To log and upload artifacts to {{objstg}} buckets through MLFlow, 
     you need to ensure that the client has access to the credentials of
     an account that can write to a bucket. This is usually settled by 
     the MLOps team, so you need only interact with MLFlow to download 
@@ -159,11 +163,21 @@
 mlflow_autolog: false
 mlflow_tracking_uri: "./mlruns"
 mlflow_exp_name: "{{cookiecutter.src_package_name_short}}"
 mlflow_run_name: "train-model"
-data_dir_path: "./data/processed"
-dummy_param1: 1.3
-dummy_param2: 0.8
+data_dir_path: "./data/processed/mnist-pngs-data-aisg-processed"
+no_cuda: true
+no_mps: true
+train_bs: 64
+test_bs: 1000
+lr: 1.0
+gamma: 0.7
+seed: 1111
+epochs: 3
+log_interval: 100
+dry_run: false
+model_checkpoint_interval: 2
+model_checkpoint_dir_path: "./models/checkpoint"
 ```
 
 After that, we build the Docker image from the Docker file 
 `docker/{{cookiecutter.repo_name}}-gpu.Dockerfile`:
@@ -212,9 +226,9 @@
 
 === "Coder Workspace Terminal using Run:ai"
 
     ```bash
-    # Run `runai login` and `runai config project {{cookiecutter.proj_name}}` first if needed
+    # Run `runai login` and `runai config project aisg` first if needed
     # Run this in the base of your project repository, and change accordingly
     # Switch working-dir to /<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}} to use the repo in the PVC
     $ runai submit \
         --job-name-prefix <YOUR_HYPHENATED_NAME>-train \
@@ -225,11 +239,14 @@
         -e MLFLOW_TRACKING_USERNAME=<YOUR_MLFLOW_USERNAME> \
         -e MLFLOW_TRACKING_PASSWORD=<YOUR_MLFLOW_PASSWORD> \
         -e OMP_NUM_THREADS=2 \
         --command -- /bin/bash -c "python -u src/train_model.py \
-            data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed \
-            artifact_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/models \
-            mlflow_tracking_uri=<MLFLOW_TRACKING_URI>"
+            data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed/mnist-pngs-data-aisg-processed \
+            setup_mlflow=true \
+            mlflow_tracking_uri=<MLFLOW_TRACKING_URI> \
+            mlflow_exp_name=<NAME_OF_DEFAULT_MLFLOW_EXPERIMENT> \
+            model_checkpoint_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}}/models \
+            epochs=3"
     ```
 
 Once you have successfully run an experiment, you may inspect the run
 on the MLflow Tracking server. Through the MLflow Tracking server
@@ -300,10 +317,10 @@
     storage: null
     n_trials: 3
     n_jobs: 1
     params:
-      dummy_param1: range(0.9,1.7,step=0.1)
-      dummy_param2: choice(0.7,0.8,0.9)
+      lr: range(0.9,1.7,step=0.1)
+      gamma: choice(0.7,0.8,0.9)
 ```
 
 These fields are used by the Optuna Sweeper plugin to configure the
 Optuna study.
@@ -328,9 +345,9 @@
 
 `src/train_model.py`
 ```python
 ...
-    return args["dummy_param1"], args["dummy_param2"]
+    return curr_test_loss, curr_test_accuracy
 ...
 ```
 
 `conf/train_model.yaml`
@@ -361,9 +378,9 @@
 
 === "Coder Workspace Terminal using Run:ai"
 
     ```bash
-    # Run `runai login` and `runai config project {{cookiecutter.proj_name}}` first if needed
+    # Run `runai login` and `runai config project aisg` first if needed
     # Run this in the base of your project repository, and change accordingly
     # Switch working-dir to /<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}} to use the repo in the PVC
     runai submit \
         --job-name-prefix <YOUR_HYPHENATED_NAME>-train-hp \
@@ -373,12 +390,16 @@
         --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \
         -e MLFLOW_TRACKING_USERNAME=<YOUR_MLFLOW_USERNAME> \
         -e MLFLOW_TRACKING_PASSWORD=<YOUR_MLFLOW_PASSWORD> \
         -e MLFLOW_HPTUNING_TAG=$(date +%s) \
+        -e OMP_NUM_THREADS=2 \
         --command -- /bin/bash -c "python -u src/train_model.py --multirun \
-            data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed \
-            artifact_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/models \
-            mlflow_tracking_uri=<MLFLOW_TRACKING_URI>"
+            data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed/mnist-pngs-data-aisg-processed \
+            setup_mlflow=true \
+            mlflow_tracking_uri=<MLFLOW_TRACKING_URI> \
+            mlflow_exp_name=<NAME_OF_DEFAULT_MLFLOW_EXPERIMENT> \
+            model_checkpoint_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}}/models \
+            epochs=3"
     ```
 
 ![MLflow Tracking Server - Hyperparameter Tuning Runs](../common/assets/screenshots/mlflow-tracking-hptuning-runs.png)
 
