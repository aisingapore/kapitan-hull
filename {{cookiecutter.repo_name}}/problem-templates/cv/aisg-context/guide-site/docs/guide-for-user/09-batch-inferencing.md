# Batch Inferencing

Some problem statements do not warrant the deployment of an API server
but instead methods for conducting batched inferencing where a batch
of data is provided to a script and it is able to churn out a set of
predictions, perhaps exported to a file.

This template provides a Python script (`src/batch_infer.py`) and a 
configuration file (`conf/batch_infer.yaml`) for this purpose. There's 

Let's first download some data on our local machine for us to conduct
batch inferencing on:

=== "Linux/macOS/VSCode Server Terminal"

    ```bash
    cd data
    wget https://storage.googleapis.com/aisg-mlops-pub-data/kapitan-hull/batched-mnist-input-data.zip
    unzip batched-mnist-input-data.zip
    rm batched-mnist-input-data.zip
    ```

=== "Windows PowerShell"

    ```powershell
    cd data
    wget https://storage.googleapis.com/aisg-mlops-pub-data/kapitan-hull/batched-mnist-input-data.zip
    Expand-Archive -Path batched-mnist-input-data.zip -DestinationPath .
    rm batched-mnist-input-data.zip
    ```

To execute the batch inferencing script locally:

=== "Linux/macOS/VSCode Server Terminal"

    ```bash
    # Navigate back to root directory
    cd "$(git rev-parse --show-toplevel)"
    export PRED_MODEL_UUID=<MLFLOW_RUN_UUID>
    export PRED_MODEL_PATH="$PWD/models/$PRED_MODEL_UUID/artifacts/model/model.pt"
    conda activate {{cookiecutter.repo_name}}
    python src/batch_infer.py
    ```

=== "Windows PowerShell"

    ```powershell
    # Navigate back to root directory
    Set-Location -Path (git rev-parse --show-toplevel)
    $Env:PRED_MODEL_UUID=<MLFLOW_RUN_UUID>
    $Env:PRED_MODEL_PATH="$(Get-Location)\models\$PRED_MODEL_UUID\artifacts\model\model.pt"
    conda activate {{cookiecutter.repo_name}}
    python src/batch_infer.py
    ```

The script will log to the terminal the location of the
`.jsonl` file (`batch-infer-res.jsonl`) containing predictions that
look like such:

```jsonl
...
{"time": "YYYY-MM-DDThh:mm:ss+0000", "image_filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png", "prediction": "X"}
{"time": "YYYY-MM-DDThh:mm:ss+0000", "image_filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png", "prediction": "X"}
{"time": "YYYY-MM-DDThh:mm:ss+0000", "image_filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png", "prediction": "X"}
{"time": "YYYY-MM-DDThh:mm:ss+0000", "image_filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png", "prediction": "X"}
{"time": "YYYY-MM-DDThh:mm:ss+0000", "image_filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png", "prediction": "X"}
...
```

The `hydra.job.chdir=True` flag writes the `.jsonl` file containing
the predictions to a subdirectory within the `outputs` folder. See 
[here] for more information on outputs generated by Hydra.

[here]: https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/