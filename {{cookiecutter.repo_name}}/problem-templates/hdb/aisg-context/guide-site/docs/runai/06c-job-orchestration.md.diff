--- {{cookiecutter.repo_name}}/aisg-context/guide-site/docs/runai/06c-job-orchestration.md
+++ {{cookiecutter.repo_name}}/problem-templates/hdb/aisg-context/guide-site/docs/runai/06c-job-orchestration.md
@@ -47,8 +47,10 @@
 raw_data_dir_path: "./data/raw"
 processed_data_dir_path: "./data/processed"
 ```
 
+There are other configurables in the `conf/process_data.yaml` which are used
+in data preparation scripts found in `src/{{cookiecutter.src_package_name}}/data_prep`.
 This requires the Docker image to be built from a Dockerfile 
 (`docker/{{cookiecutter.src_package_name}}-cpu.Dockerfile`)
 provided in this template:
 
@@ -160,10 +162,15 @@
 mlflow_tracking_uri: "./mlruns"
 mlflow_exp_name: "{{cookiecutter.src_package_name_short}}"
 mlflow_run_name: "train-model"
 data_dir_path: "./data/processed"
-dummy_param1: 1.3
-dummy_param2: 0.8
+artifact_dir_path: "./models"
+use_cuda: false
+n_estimators: 100
+lr: 0.5
+gamma: 1
+max_depth: 5
+seed: 1111
 ```
 
 After that, we build the Docker image from the Docker file 
 `docker/{{cookiecutter.repo_name}}-gpu.Dockerfile`:
@@ -223,13 +230,15 @@
         --existing-pvc claimname=<NAME_OF_DATA_SOURCE>,path=/<NAME_OF_DATA_SOURCE> \
         --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \
         -e MLFLOW_TRACKING_USERNAME=<YOUR_MLFLOW_USERNAME> \
         -e MLFLOW_TRACKING_PASSWORD=<YOUR_MLFLOW_PASSWORD> \
+        -e OMP_NUM_THREADS=2 \
         --command -- /bin/bash -c "python -u src/train_model.py \
             mlflow_tracking_uri=<MLFLOW_TRACKING_URI> \
             mlflow_exp_name=<NAME_OF_DEFAULT_MLFLOW_EXPERIMENT> \
             data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed \
-            artifact_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/models"
+            model_checkpoint_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}}/models \
+            epochs=3"
     ```
 
 Once you have successfully run an experiment, you may inspect the run
 on the MLflow Tracking server. Through the MLflow Tracking server
@@ -294,16 +303,18 @@
 hydra:
   sweeper:
     sampler:
       seed: 55
-    direction: ["minimize", "maximize"]
-    study_name: "image-classification"
+    direction: ["minimize"]
+    study_name: "hdb-resale-process"
     storage: null
     n_trials: 3
     n_jobs: 1
     params:
-      dummy_param1: range(0.9,1.7,step=0.1)
-      dummy_param2: choice(0.7,0.8,0.9)
+      n_estimators: range(50, 200, step=10)
+      lr: tag(log, interval(0.1, 0.6))
+      gamma: choice(0,0.1,0.2,0.3,0.4,0.5)
+      max_depth: range(2,20,step=1)
 ```
 
 These fields are used by the Optuna Sweeper plugin to configure the
 Optuna study.
@@ -328,22 +339,22 @@
 
 `src/train_model.py`
 ```python
 ...
-    return args["dummy_param1"], args["dummy_param2"]
+    return test_rmse ## or any other metrics
 ...
 ```
 
 `conf/train_model.yaml`
 ```yaml
 ...
-    direction: ["minimize", "maximize"]
+    direction: ["minimize"] ## or ["maximise"], if you're looking to maximise the test_rmse value
 ...
 ```
 
 In the training script the returned variables are to contain values
 that we seek to optimise for. In this case, we seek to minimise the 
-loss and maximise the accuracy. The `hydra.sweeper.direction` field in 
+root mean square error. The `hydra.sweeper.direction` field in 
 the YAML config is used to specify the direction that those variables 
 are to optimise towards, defined in a positional manner within a list.
 
 An additional thing to take note of is that for each trial where a
@@ -373,13 +384,15 @@
         --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \
         -e MLFLOW_TRACKING_USERNAME=<YOUR_MLFLOW_USERNAME> \
         -e MLFLOW_TRACKING_PASSWORD=<YOUR_MLFLOW_PASSWORD> \
         -e MLFLOW_HPTUNING_TAG=$(date +%s) \
+        -e OMP_NUM_THREADS=2 \
         --command -- /bin/bash -c "python -u src/train_model.py --multirun \
             mlflow_tracking_uri=<MLFLOW_TRACKING_URI> \
             mlflow_exp_name=<NAME_OF_DEFAULT_MLFLOW_EXPERIMENT> \
             data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed \
-            artifact_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/models"
+            model_checkpoint_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}}/models \
+            epochs=3"
     ```
 
 ![MLflow Tracking Server - Hyperparameter Tuning Runs](../common/assets/screenshots/mlflow-tracking-hptuning-runs.png)
 
