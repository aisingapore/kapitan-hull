--- {{cookiecutter.repo_name}}/aisg-context/guide-site/docs/runai/06c-job-orchestration.md
+++ {{cookiecutter.repo_name}}/problem-templates/hdb/aisg-context/guide-site/docs/runai/06c-job-orchestration.md
@@ -49,4 +49,6 @@
 ```
 
+There are other configurables in the `conf/process_data.yaml` which are used
+in data preparation scripts found in `src/{{cookiecutter.src_package_name}}/data_prep`.
 This requires the Docker image to be built from a Dockerfile 
 (`docker/{{cookiecutter.src_package_name}}-cpu.Dockerfile`)
@@ -92,5 +94,5 @@
 
     ```bash
-    # Run `runai login` and `runai config project {{cookiecutter.proj_name}}` first if needed
+    # Run `runai login` and `runai config project aisg` first if needed
     # Run this in the base of your project repository, and change accordingly
     # Switch working-dir to /<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}} to use the repo in the PVC
@@ -162,6 +164,11 @@
 mlflow_run_name: "train-model"
 data_dir_path: "./data/processed"
-dummy_param1: 1.3
-dummy_param2: 0.8
+artifact_dir_path: "./models"
+use_cuda: false
+n_estimators: 100
+lr: 0.5
+gamma: 1
+max_depth: 5
+seed: 1111
 ```
 
@@ -214,5 +221,5 @@
 
     ```bash
-    # Run `runai login` and `runai config project {{cookiecutter.proj_name}}` first if needed
+    # Run `runai login` and `runai config project aisg` first if needed
     # Run this in the base of your project repository, and change accordingly
     # Switch working-dir to /<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}} to use the repo in the PVC
@@ -228,6 +235,9 @@
         --command -- /bin/bash -c "python -u src/train_model.py \
             data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed \
-            artifact_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/models \
-            mlflow_tracking_uri=<MLFLOW_TRACKING_URI>"
+            setup_mlflow=true \
+            mlflow_tracking_uri=<MLFLOW_TRACKING_URI> \
+            mlflow_exp_name=<NAME_OF_DEFAULT_MLFLOW_EXPERIMENT> \
+            model_checkpoint_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}}/models \
+            epochs=3"
     ```
 
@@ -296,12 +306,14 @@
     sampler:
       seed: 55
-    direction: ["minimize", "maximize"]
-    study_name: "image-classification"
+    direction: ["minimize"]
+    study_name: "hdb-resale-process"
     storage: null
     n_trials: 3
     n_jobs: 1
     params:
-      dummy_param1: range(0.9,1.7,step=0.1)
-      dummy_param2: choice(0.7,0.8,0.9)
+      n_estimators: range(50, 200, step=10)
+      lr: tag(log, interval(0.1, 0.6))
+      gamma: choice(0,0.1,0.2,0.3,0.4,0.5)
+      max_depth: range(2,20,step=1)
 ```
 
@@ -330,5 +342,5 @@
 ```python
 ...
-    return args["dummy_param1"], args["dummy_param2"]
+    return test_rmse ## or any other metrics
 ...
 ```
@@ -337,5 +349,5 @@
 ```yaml
 ...
-    direction: ["minimize", "maximize"]
+    direction: ["minimize"] ## or ["maximise"], if you're looking to maximise the test_rmse value
 ...
 ```
@@ -343,5 +355,5 @@
 In the training script the returned variables are to contain values
 that we seek to optimise for. In this case, we seek to minimise the 
-loss and maximise the accuracy. The `hydra.sweeper.direction` field in 
+root mean square error. The `hydra.sweeper.direction` field in 
 the YAML config is used to specify the direction that those variables 
 are to optimise towards, defined in a positional manner within a list.
@@ -363,5 +375,5 @@
 
     ```bash
-    # Run `runai login` and `runai config project {{cookiecutter.proj_name}}` first if needed
+    # Run `runai login` and `runai config project aisg` first if needed
     # Run this in the base of your project repository, and change accordingly
     # Switch working-dir to /<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}} to use the repo in the PVC
@@ -375,8 +387,12 @@
         -e MLFLOW_TRACKING_PASSWORD=<YOUR_MLFLOW_PASSWORD> \
         -e MLFLOW_HPTUNING_TAG=$(date +%s) \
+        -e OMP_NUM_THREADS=2 \
         --command -- /bin/bash -c "python -u src/train_model.py --multirun \
             data_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/data/processed \
-            artifact_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/models \
-            mlflow_tracking_uri=<MLFLOW_TRACKING_URI>"
+            setup_mlflow=true \
+            mlflow_tracking_uri=<MLFLOW_TRACKING_URI> \
+            mlflow_exp_name=<NAME_OF_DEFAULT_MLFLOW_EXPERIMENT> \
+            model_checkpoint_dir_path=/<NAME_OF_DATA_SOURCE>/workspaces/<YOUR_HYPHENATED_NAME>/{{cookiecutter.repo_name}}/models \
+            epochs=3"
     ```
 
