--- {{cookiecutter.repo_name}}/src/train_model.py
+++ {{cookiecutter.repo_name}}/problem-templates/hdb/src/train_model.py
@@ -2 +2,2 @@
-This script is for training a dummy model on a dummy dataset.
+This script is for training a XGBRegressor model on the HDB dataset. Note that
+the script is incomplete and requires you to modify it accordingly to your solution.
@@ -34,0 +36,11 @@
+    
+    params = {
+        "n_estimators": args["n_estimators"],
+        "learning_rate": args["lr"],
+        "gamma": args["gamma"],
+        "max_depth": args["max_depth"],
+    }
+
+    if use_cuda := args["use_cuda"]:
+       params["device"] = cuda
+       params["tree_method"] = "hist"     
@@ -38,4 +50 @@
-        params={
-            "dummy_param1": args["dummy_param1"],
-            "dummy_param2": args["dummy_param2"],
-        },
+        params=params,
@@ -44,2 +53,6 @@
-    model = {{cookiecutter.src_package_name_short}}.modeling.models.DummyModel()
-
+    model = {{cookiecutter.src_package_name_short}}.modeling.models.Model()
+    
+    ### Insert your model code
+    # train_score = model.train(params, X_train, y_train, mlflow_init_status)
+    # test_rmse = model.evaluate()
+    
@@ -55 +68 @@
-        args["artifact_dir_path"], "output.txt"
+        args["artifact_dir_path"], "xgbreg.json"
@@ -56,0 +70,3 @@
+    
+    # model.save_model(os.path.join(args["artifact_dir_path"], "xgbreg.json"))
+    
@@ -63 +79 @@
-        artifact_path="outputs"
+        artifact_path="model"
@@ -80,2 +96,4 @@
-    # Outputs for conf/train_model.yaml for hydra.sweeper.direction
-    return args["dummy_param1"], args["dummy_param2"]
+    ## Uncomment return line
+    ## The function decorated with @hydra.main() returns a value that we want to
+    ## adjust base on hydra.sweeper.direction in conf/train_model.yaml
+    # return test_rmse
