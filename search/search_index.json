{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Project Template","text":"<p>End-to-end template for AI projects</p> <p>A project generated using AI Singapore's Kapitan Hull, an end-to-end  ML project template.</p> <p>This template that is also accompanied with an end-to-end guide was generated and customised using the following <code>cookiecutter</code> template:</p> <p>https://github.com/aisingapore/kapitan-hull</p> <p>This <code>mkdocs</code> site is for serving the contents of the end-to-end guide  in a more readable manner, as opposed to plain Markdown views. The  contents of this guide have been unified from multiple templates to provide a comprehensive reference for AI projects.</p>"},{"location":"#directory-tree","title":"Directory Tree","text":"<pre><code>project-repo\n\u251c\u2500\u2500 aisg-context        &lt;- Folders containing files and assets relevant\n\u2502   \u2502                      for works within the context of AISG's\n\u2502   \u2502                      development environments.\n\u2502   \u2514\u2500\u2500 guide-site      &lt;- Files relevant for spinning up the `mkdocs`\n\u2502                          site to view the end-to-end guide.\n\u251c\u2500\u2500 conf                &lt;- Configuration files associated with the\n\u2502                          various pipelines as well as for logging.\n\u251c\u2500\u2500 data                &lt;- Folder to contain any data for the various\n\u2502                          pipelines. Ignored by Git except its\n\u2502                          `.gitkeep` file.\n\u251c\u2500\u2500 docker              &lt;- Dockerfiles associated with the various\n\u2502                          stages of the pipeline.\n\u251c\u2500\u2500 docs                &lt;- A default Sphinx project; see sphinx-doc.org\n\u2502                          for details.\n\u251c\u2500\u2500 models              &lt;- Directory for trained and serialised models.\n\u251c\u2500\u2500 notebooks           &lt;- Jupyter notebooks. Suggested naming\n\u2502                          convention would be number (for ordering),\n\u2502                          the creator's initials, and a short `-`\n\u2502                          delimited description, e.g.\n\u2502                          `1.0-jqp-initial-data-exploration`.\n\u251c\u2500\u2500 src                 &lt;- Directory containing the source code and\n|   |                      packages for the project repository.\n\u2502   \u251c\u2500\u2500 project_package\n\u2502   \u2502                   ^- Package containing modules for all pipelines \n\u2502   \u2502                      except deployment of API server.\n\u2502   \u251c\u2500\u2500 project_package_fastapi\n\u2502   \u2502   ^- Package for deploying the predictive models within a FastAPI\n\u2502   \u2502      server.\n\u2502   \u2514\u2500\u2500 tests           &lt;- Directory containing tests for the\n\u2502                          repository's packages.\n\u251c\u2500\u2500 .dockerignore       &lt;- File for specifying files or directories\n\u2502                          to be ignored by Docker contexts.\n\u251c\u2500\u2500 .gitignore          &lt;- File for specifying files or directories\n\u2502                          to be ignored by Git.\n\u251c\u2500\u2500 .gitlab-ci.yml      &lt;- YAML file for configuring GitLab CI/CD\n\u2502                          pipelines.\n\u251c\u2500\u2500 .pylintrc           &lt;- Configurations for `pylint`.\n\u251c\u2500\u2500 project-conda-env.yaml\n\u2502                       ^- The `conda` environment file for reproducing\n\u2502                          the project's development environment.\n\u2514\u2500\u2500 README.md           &lt;- The top-level README containing the basic\n                           guide for using the repository.\n</code></pre>"},{"location":"appendix/09-cicd/","title":"Continuous Integration &amp; Deployment","text":"<p>This template presents users with a base configuration for a GitLab  CI/CD pipeline. In this section, the guide aims to provide readers with some basic understanding of the pipeline defined in the configuration file <code>.gitlab-ci.yml</code>.</p> <p>That being said, readers would certainly benefit from reading up on introductory CI/CD concepts as introduced by GitLab's  Docs.</p>"},{"location":"appendix/09-cicd/#github-flow","title":"GitHub Flow","text":"<p>The defined pipeline assumes a GitHub flow which only relies on feature  branches and a <code>main</code> (default) branch.</p> <p></p> <p>With reference to the diagram above, we have the following pointers:</p> <ul> <li>We make use of feature branches (<code>git checkout -b &lt;NAME_OF_BRANCH&gt;</code>)    to introduce changes to the source.</li> <li>Merge requests are made when we intend to merge the commits made to a   feature branch to <code>main</code>.</li> <li>While one works on a feature branch, it is recommended that changes   pushed to the <code>main</code> are pulled to the feature branch itself on a   consistent basis. This allows the feature branch to possess the   latest changes pushed by other developers through their own feature   branches. In the example above, commits from the <code>main</code> branch   following a merge of the <code>add-hidden-layer</code> branch are pulled into   the <code>change-training-image</code> branch while that branch still expects   further changes.</li> <li>The command <code>git pull</code> can be used to pull and sync these changes.    However, it's recommended that developers make use of <code>git fetch</code> and    <code>git log</code> to observe incoming changes first rather than pulling in    changes in an indiscriminate manner.</li> <li>While it's possible for commits to be made directly to the <code>main</code>    branch, it's recommended that they are kept minimal, at least for    GitHub flow (other workflows might not heed such practices).</li> </ul> <p>As we move along, we should be able to relate parts of the flow described above with the stages defined by the default GitLab CI pipeline.</p> <p>For more information on Gitlab CI pipeline, you can refer  here (AISG personnel only).</p>"},{"location":"appendix/09-cicd/#environment-variables","title":"Environment Variables","text":"<p>Before we can make use of the GitLab CI pipeline, we would have to define the following variable(s) for the pipeline beforehand, depending on your project's requirements:</p> <ul> <li> <p><code>HARBOR_ROBOT_CREDS_JSON</code>: A JSON formatted value that contains   encoded credentials for a robot account on Harbor. This is to allow   the pipeline to interact with the Harbor server. See the next section    on how to generate this value/file.  </p> </li> <li> <p><code>GCP_SERVICE_ACCOUNT_KEY</code>: A JSON formatted value that contains    encoded credentials for a service account on your GCP project. This    is to allow the pipeline to interact with the Google Artifact    Registry. See here on how to generate this file.  </p> </li> </ul> <p>For both variables, after you've generated the JSON file, please encode  the file content using <code>base64 -i &lt;file&gt;</code>. Afterwhich, copy paste the  encoded value and define it as a CI/CD variable. </p> <p>To define CI/CD variables for a project (repository), follow the steps listed here. </p> <p>Both environment variables <code>HARBOR_ROBOT_CREDS_JSON</code> and  <code>GCP_SERVICE_ACCOUNT_KEY</code> need to be <code>variable</code> types.</p> <p>After defining the CI/CD variables for the project, your pipeline should be able to pass. If not, re-run the pipeline.</p>"},{"location":"appendix/09-cicd/#docker-configuration-file-for-accessing-harbor","title":"Docker Configuration File for Accessing Harbor","text":"<p>The variable <code>HARBOR_ROBOT_CREDS_JSON</code> will be used to populate the files <code>/kaniko/.docker/config.json</code> and <code>/root/.docker/config.json</code> for <code>kaniko</code> and <code>crane</code> to authenticate themselves before communicating with AI Singapore's Harbor registry. You may create the JSON file like so:</p> Linux/macOSWindows PowerShell <pre><code>echo -n &lt;HARBOR_USERNAME&gt;:&lt;HARBOR_PASSWORD&gt; | base64\n</code></pre> <pre><code>$Env:cred = \"&lt;HARBOR_USERNAME&gt;:&lt;HARBOR_PASSWORD&gt;\"\n$Env:bytes = [System.Text.Encoding]::ASCII.GetBytes($cred)\n$Env:base64 = [Convert]::ToBase64String($bytes)\necho $base64\n</code></pre> <p>Using the output from above, copy and paste the following content into a CI/CD environment variable of type <code>File</code> (under <code>Settings</code> -&gt; <code>CI/CD</code> -&gt; <code>Variables</code> -&gt; <code>Add variable</code>):</p> <pre><code>{\n    \"auths\": {\n        \"registry.aisingapore.net\": {\n            \"auth\": \"&lt;ENCODED_OUTPUT_HERE&gt;\"\n        }\n    }\n}\n</code></pre> <p></p> <p>After defining the CI/CD Variables for the project, your pipeline  should be able to pass. If not, re-run the pipeline. </p> Reference Link(s) <ul> <li>GitLab Docs - GitLab CI/CD variables</li> <li>Docker Docs - Configuration files</li> </ul>"},{"location":"appendix/09-cicd/#stages-jobs","title":"Stages &amp; Jobs","text":"<p>In the default pipeline, we have 3 stages defined:</p> <ul> <li><code>test</code>: For every push to certain branches, the source code residing   in <code>src</code> will be tested.</li> <li><code>deploy-docs</code>: This stage is for the purpose of deploying a static   site through GitLab Pages. More on this stage is covered in    \"Documentation\".</li> <li><code>build</code>: Assuming the automated tests are passed, the pipeline   will build Docker images, making use of the latest source.</li> </ul> <p>These stages are defined and listed like so:</p> <code>.gitlab-ci.yml</code> <pre><code>...\nstages:\n  - build\n  - test\n  - deploy\n  - deploy-docs\n...\n</code></pre> <p>The jobs for each of the stages are executed using Docker images  defined by users. For this, we have to specify in the pipeline the tag associated with the GitLab Runner that has the Docker executor.  The <code>on-prem</code> tag calls for runners within our on-premise  infrastructure so on-premise services can be accessed within our  pipelines. The <code>gcp</code> tag calls for runners on our GCP infrastructure so it can use the GCP services within our pipelines.</p> <p>The <code>./conda</code> folder generated from creating the Conda environment is  then cached and to be used for other jobs, saving time from rebuilding  the environment in every job that requires it. The  <code>$CI_COMMIT_REF_SLUG</code> key refers to the branch name modified to be  code-friendly. In this case, it is used as a namespace to store all the files that is cached within this branch.</p> <code>.gitlab-ci.yml</code> <pre><code>default:\n  tags:\n    - on-prem # or `gcp`\n...\n</code></pre>"},{"location":"appendix/09-cicd/#variables","title":"Variables","text":"<p>The GitLab CI pipeline uses several variables to control its behavior:</p> <ul> <li><code>PYTHON_IMAGE</code>: Specifies the Docker image used for Python-based jobs (default: <code>continuumio/miniconda3:24.7.1-0</code>)</li> <li><code>VENV_DIRECTORY</code>: Defines the path where the Conda environment will be created and stored</li> <li><code>IMAGE_TAG</code>: Default tag for Docker images (default: <code>latest</code>)</li> <li><code>BUILD_CONDA</code>: When set in a manual pipeline run, forces the Conda environment to be built</li> <li><code>BUILD_ALL</code>: When set in a manual pipeline run, triggers building of all Docker images</li> <li><code>BUILD_DATAPREP</code>: When set in a manual pipeline run, triggers building of the data preparation image</li> <li><code>BUILD_MODEL</code>: When set in a manual pipeline run, triggers building of the model training image</li> </ul> <p>GitLab also provides many predefined variables that are used in the pipeline: - <code>CI_COMMIT_REF_SLUG</code>: Branch or tag name in a URL-friendly format - <code>CI_COMMIT_SHORT_SHA</code>: The first 8 characters of the commit SHA - <code>CI_DEFAULT_BRANCH</code>: The default branch for the project (usually <code>main</code>) - <code>CI_PIPELINE_SOURCE</code>: How the pipeline was triggered (e.g., \"push\", \"web\", \"merge_request_event\") - <code>CI_PROJECT_DIR</code>: The full path where the repository is cloned - <code>CI_MERGE_REQUEST_IID</code>: The merge request ID if the pipeline is for a merge request - <code>CI_COMMIT_TAG</code>: The commit tag name if the pipeline was triggered by a tag</p>"},{"location":"appendix/09-cicd/#building-the-conda-environment","title":"Building the Conda Environment","text":"<p>Let's look at the job defined for the <code>build</code> stage first:</p> <code>.gitlab-ci.yml</code> <pre><code>...\nbuild:conda-env:\n  stage: build\n  image:\n    name: ${PYTHON_IMAGE}\n  script:\n    - conda env create -f project-conda-env.yaml -p ${VENV_DIRECTORY}\n  rules:\n    - if: $CI_MERGE_REQUEST_IID\n      changes:\n        - project-conda-env.yaml\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n      changes:\n        - project-conda-env.yaml\n    - if: $CI_PIPELINE_SOURCE == \"web\"\n      changes:\n        - project-conda-env.yaml\n    - if: $CI_PIPELINE_SOURCE == \"web\" &amp;&amp; $BUILD_CONDA\n    - if: $CI_COMMIT_TAG\n      when: never\n  needs: []\n...\n</code></pre> <p>First of all, this <code>build:conda-env</code> job will only execute on the condition that the defined <code>rules</code> are met. In this case, the job will only execute for the following cases:</p> <ul> <li>For pushes to branches which merge requests have been created, tests   are executed only if changes made to the    <code>project-conda-env.yaml</code> are detected. This is to    prevent automated tests from running for pushes made to feature    branches with merge requests when  no changes have been made to files    for which tests are relevant. Otherwise, tests will run in a    redundant manner, slowing down the feedback loop.</li> <li>For any pushes to any branch.</li> <li>For any manual pipeline execution through GitLab's web UI.</li> <li>If the push action is associated with a tag   (<code>git push &lt;remote&gt; &lt;tag_name&gt;</code>), the job will not run.</li> </ul> <p>The job does not have any jobs that it needs to wait for, thus the  <code>needs</code> section is populated with <code>[]</code>.</p>"},{"location":"appendix/09-cicd/#automated-testing-linting","title":"Automated Testing &amp; Linting","text":"<p>The <code>test</code> stage includes two separate jobs: one for linting and one for running tests:</p> <code>.gitlab-ci.yml</code> - Linting Job<code>.gitlab-ci.yml</code> - Testing Job <pre><code>...\ntest:lint:\n  stage: test\n  before_script:\n    - source activate ${VENV_DIRECTORY}\n    - pip install -r dev-requirements.txt\n  script:\n    - pylint src --fail-under=7.0 --ignore=tests --disable=W1202\n  rules:\n    - if: $CI_MERGE_REQUEST_IID\n      changes:\n        - src/**/*\n        - conf/**/*\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n    - if: $CI_PIPELINE_SOURCE == \"web\"\n    - if: $CI_COMMIT_TAG\n      when: never\n  needs:\n    - job: build:conda-env\n      optional: true\n...\n</code></pre> <pre><code>...\ntest:pytest:\n  stage: test\n  before_script: \n    - source activate ${VENV_DIRECTORY}\n    - pip install -r dev-requirements.txt\n  script:\n    - pytest src/tests --junitxml=./rspec.xml\n  rules:\n    - if: $CI_MERGE_REQUEST_IID\n      changes:\n        - src/**/*\n        - conf/**/*\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n    - if: $CI_PIPELINE_SOURCE == \"web\"\n    - if: $CI_COMMIT_TAG\n      when: never\n  artifacts:\n    paths:\n      - rspec.xml\n    reports:\n      junit: rspec.xml\n  needs:\n    - job: build:conda-env\n      optional: true\n...\n</code></pre> <p>For both the <code>test:lint</code> and <code>test:pytest</code> jobs, they will only execute for the following cases:</p> <ul> <li>For pushes to branches which merge requests have been created, tests   are executed only if changes made to any files within <code>src</code> or <code>conf</code>    are detected.</li> <li>For any pushes to any branch.</li> <li>For any manual pipeline execution through GitLab's web UI.</li> <li>If the push action is associated with a tag   (<code>git push &lt;remote&gt; &lt;tag_name&gt;</code>), the job will not run.</li> </ul> <p>Both jobs wait for <code>build:conda-env</code> to be completed first before they  can be executed. The <code>optional: true</code> option is set so that they would  still run if the <code>build:conda-env</code> job doesn't since the environment  has already been cached to be used in these jobs.</p> <p>The <code>test:lint</code> job fails if the source code does not meet a linting  score of at least 7.0.</p> <p>The <code>test:pytest</code> job fails if the source code fails any tests that  have been defined under <code>src/tests</code>. This job generates a <code>rspec.xml</code>  file as an artifact so that you can read the test results in the GitLab  UI. More information about this can be found here.</p> <p>Both jobs would have to succeed before moving on to the <code>deploy</code> stage. Otherwise, no Docker images will be built. This is so that source code that fails either linting or tests would never be packaged.</p> Reference Link(s) <ul> <li>GitLab Docs - Predefined variables reference</li> <li>Real Python - Effective Python Testing With Pytest</li> <li>VSCode Docs - Linting Python in Visual Studio Code</li> </ul>"},{"location":"appendix/09-cicd/#automated-builds","title":"Automated Builds","text":"<p>The template has thus far introduced a couple of Docker images relevant for the team. The tags for all the Docker images are listed below:</p> <ul> <li><code>registry.aisingapore.net/project-path/cpu</code></li> <li><code>registry.aisingapore.net/project-path/gpu</code></li> </ul> <p>The <code>deploy</code> stage aims at automating the building of these Docker images in a parallel manner. Let's look at a snippet for a single job that builds a Docker image:</p> <code>.gitlab-ci.yml</code> <pre><code>...\nbuild:cpu-image:\n  stage: deploy\n  image:\n    name: gcr.io/kaniko-project/executor:debug\n    entrypoint: [\"\"]\n    variables:\n      GOOGLE_APPLICATION_CREDENTIALS: /kaniko/.docker/config.json # For GCP\n  before_script:\n    - \"[[ -z ${HARBOR_ROBOT_CREDS_JSON} ]] &amp;&amp; echo 'HARBOR_ROBOT_CREDS_JSON variable is not set.' &amp;&amp; exit 1\" # For onprem\n    - \"[[ -z ${GCP_SERVICE_ACCOUNT_KEY} ]] &amp;&amp; echo 'GCP_SERVICE_ACCOUNT_KEY variable is not set.' &amp;&amp; exit 1\" # For GCP\n  script:\n    - mkdir -p /kaniko/.docker\n    - cat $HARBOR_ROBOT_CREDS_JSON &gt; /kaniko/.docker/config.json # For onprem\n    - cat $GCP_SERVICE_ACCOUNT_KEY &gt; /kaniko/.docker/config.json # For GCP\n    - &gt;-\n      /kaniko/executor\n      --context \"${CI_PROJECT_DIR}\"\n      --dockerfile \"${CI_PROJECT_DIR}/docker/project-cpu.Dockerfile\"\n      --destination \"registry.aisingapore.net/project-path/cpu:${CI_COMMIT_SHORT_SHA}\"\n  rules:\n    - if: $CI_MERGE_REQUEST_IID\n      changes:\n        - docker/project-cpu.Dockerfile\n        - src/**/*\n        - conf/**/*\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n    - if: $CI_PIPELINE_SOURCE == \"web\" &amp;&amp; $BUILD_ALL\n    - if: $CI_PIPELINE_SOURCE == \"web\" &amp;&amp; $BUILD_DATAPREP\n  needs:\n    - job: test:lint\n      optional: true\n    - job: test:pytest\n      optional: true\n...\n</code></pre> <p>Note</p> <p>You would have noticed that the jobs for building images utilise the command <code>/kaniko/executor</code> as opposed to <code>docker build</code> which most users would be more familiar with. This is due to the usage of <code>kaniko</code> within a runner with a Docker executor. Using  Docker within Docker (Docker-in-Docker) requires privileged  mode that poses several security concerns. Hence, the image  <code>gcr.io/kaniko-project/executor:debug</code> is being used for all  <code>deploy</code> jobs related to building of Docker images. That being said,  the flags used for <code>kaniko</code> corresponds well with the flags usually  used for <code>docker</code> commands.</p> <p>Before it goes through the job, it will check whether either <code>HARBOR_ROBOT_CREDS_JSON</code> and <code>GCP_SERVICE_ACCOUNT_KEY</code> have been set in the CI/CD variables.  Otherwise, it will prematurely stop the job with the error, preventing  the job from running any further and freeing the CI worker faster to  work on other jobs in the organisation.</p> <p>Just like with the <code>test</code> jobs, the each of the jobs under <code>deploy</code> will execute under certain conditions:</p> <ul> <li>If a push is being done to a branch which has a merge request opened,   a check would be done to see if any changes were made to folders like   <code>src</code>, <code>conf</code>, <code>scripts</code>, or the relevant Dockerfile itself. If there   are changes, the job will be executed. An opened merge request is   detected through the predefined variable <code>CI_MERGE_REQUEST_IID</code>.</li> <li>If a push is being made to the default branch (<code>CI_DEFAULT_BRANCH</code>)   of the repo, which in most cases within our organisation would be    <code>main</code>, the job would execute as well. Recalling the <code>test</code> stage,    any pushes to the repo would trigger the automated tests and linting.    If a push to the <code>main</code> branch passes the tests, all Docker images    will be built, regardless of whether changes have been made to files   relevant to the Docker images to be built themselves.</li> <li>For any manual pipeline execution through GitLab's web UI that has    either <code>BUILD_ALL</code> or <code>BUILD_DATAPREP</code> (or <code>BUILD_MODEL</code> for the    model training image) variable has been set. It can be set to any    value, but we can set it to <code>true</code> by default.</li> </ul> <p>The jobs in the <code>deploy</code> stage requires the <code>test:lint</code> and  <code>test:pytest</code> jobs to be successful, otherwise it would not run.</p> <p>Images built through the pipeline will be tagged with the commit hashes associated with the commits that triggered it. This is seen through the usage of the predefined variable <code>CI_COMMIT_SHORT_SHA</code>.</p> Reference Link(s) <ul> <li>GitLab Docs - Use kaniko to build Docker images</li> <li>GitLab Docs - Use Docker to build Docker images</li> </ul>"},{"location":"appendix/09-cicd/#tagging","title":"Tagging","text":"<p>As mentioned, pushes to the default branch would trigger builds for Docker images and they would be tagged with the commit hash. However, such commit hashes aren't the best way to tag \"finalised\" Docker images so the usage of tags would be more appropriate here. Hence, for the job defined below, it would only trigger if a tag is pushed to the default branch and only the default branch. The tag pushed (<code>git push &lt;remote&gt;  &lt;tag&gt;</code>) to the default branch on the remote would have the runner retag the Docker image that exists on Harbor with the tag that is being pushed. The relevant images to be retagged are originally tagged with the short commit hash obtained from the commit that was pushed to the default branch before this.</p> <code>.gitlab-ci.yml</code> <pre><code>...\nbuild:retag-images:\n  stage: deploy\n  image:\n    name: gcr.io/go-containerregistry/crane:debug # google/cloud-sdk:debian_component_based for GCP\n    entrypoint: [\"\"]\n  variables:\n    GOOGLE_APPLICATION_CREDENTIALS: /gcp-sa.json # For GCP\n  before_script:\n    - \"[[ -z ${HARBOR_ROBOT_CREDS_JSON} ]] &amp;&amp; echo 'HARBOR_ROBOT_CREDS_JSON variable is not set.' &amp;&amp; exit 1\" # For onprem\n    - \"[[ -z ${GCP_SERVICE_ACCOUNT_KEY} ]] &amp;&amp; echo 'GCP_SERVICE_ACCOUNT_KEY variable is not set.' &amp;&amp; exit 1\" # For GCP\n  script:\n    - cat $HARBOR_ROBOT_CREDS_JSON &gt; /root/.docker/config.json\n    - crane tag registry.aisingapore.net/project-path/cpu:${CI_COMMIT_SHORT_SHA} ${CI_COMMIT_TAG}\n    - crane tag registry.aisingapore.net/project-path/gpu:${CI_COMMIT_SHORT_SHA} ${CI_COMMIT_TAG}\n    # For GCP:\n    #- cat $GCP_SERVICE_ACCOUNT_KEY &gt; /gcp-sa.json\n    #- gcloud container images add-tag \"gcr.io/project/project-path/cpu:${CI_COMMIT_SHORT_SHA}\" \"gcr.io/project/project-path/cpu:${CI_COMMIT_TAG}\"\n    #- gcloud container images add-tag \"gcr.io/project/project-path/gpu:${CI_COMMIT_SHORT_SHA}\" \"gcr.io/project/project-path/gpu:${CI_COMMIT_TAG}\"\n  rules:\n    - if: $CI_COMMIT_TAG &amp;&amp; $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n  needs:\n    - job: build:cpu-image\n      optional: true\n    - job: build:gpu-image\n      optional: true\n...\n</code></pre> Reference Link(s) <ul> <li>GitHub Docs - GitHub Flow</li> <li>GitLab Docs - GitLab Flow</li> <li><code>go-containerregistry</code> GitHub - <code>crane</code></li> </ul>"},{"location":"appendix/09-cicd/#conclusion","title":"Conclusion","text":"<p>The stages and jobs defined in this default pipeline is rudimentary at best as there is much more that could be done with GitLab CI. Some examples off the top:</p> <ul> <li>automatically generate reports for datasets that arrive in regular   intervals</li> <li>submit model training jobs following triggers invoked by the same   pipeline</li> <li>automate the deployment of the FastAPI servers to Kubernetes clusters</li> </ul> <p>There's much more that can be done but whatever has been shared thus  far is hopefully enough for one to get started with CI/CD. </p> <p>Maintaining CI/CD pipelines requires extensive effort from developers.  To reduce the effort required from developers, the MLOps Team has  written a set of templates in which users can implement - plug and play  with CI/CD Components.</p>"},{"location":"appendix/10-documentation/","title":"Documentation","text":"<p>The boilerplate packages generated by the template are populated with some NumPy formatted docstrings. What we can do with this is  to observe how documentation can be automatically generated using Sphinx, with the aid of the Napoleon extension. Let's build the  HTML asset for the documentation:</p> <pre><code># From the root folder\nconda activate ghpages\nsphinx-apidoc -f -o docs src\nsphinx-build -b html docs public\n</code></pre> <p>Open the file <code>public/index.html</code> with your browser and you will be presented with a static site similar to the one shown below:</p> <p></p> <p>Browse through the site and inspect the documentation that was automatically generated through Sphinx.</p>"},{"location":"appendix/10-documentation/#gitlab-pages","title":"GitLab Pages","text":"<p>Documentation generated through Sphinx can be served on GitLab Pages,  through GitLab CI/CD. With this template, a default CI job has been  defined in <code>.gitlab-ci.yml</code> to serve the Sphinx documentation when  pushes are done to the <code>main</code> branch:</p> <pre><code>...\npages:\n  stage: deploy-docs\n  before_script:\n    - source activate ${VENV_DIRECTORY}\n    - pip install -r docs-requirements.txt\n    - pip install -r aisg-context/guide-site/mkdocs-requirements.txt\n  script:\n    - sphinx-apidoc -f -o docs src\n    - sphinx-build -b html docs public\n    - mkdocs build -v -f aisg-context/guide-site/mkdocs.yml -d $PWD/public/guide\n  artifacts:\n    paths:\n    - public\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      changes:\n        - docs/**/*\n        - src/**/*\n        - aisg-context/guide-site/**/*\n  needs:\n    - job: build:conda-env\n      optional: true\n...\n</code></pre> <p>The documentation page is viewable through the following convention: <code>&lt;NAMESPACE&gt;.gitlab.example.com/&lt;PROJECT_NAME&gt;</code> or <code>&lt;NAMESPACE&gt;.gitlab.example.com/&lt;GROUP&gt;/&lt;PROJECT_NAME&gt;</code>.</p> Reference Link(s) <ul> <li>GitLab Docs - Pages domain names, URLs, and base URLs</li> <li>GitLab Docs - Namespaces</li> </ul>"},{"location":"docker/03b-dev-wksp/","title":"Development Workspace","text":"<p>You can run VSCode in Docker if you feel like doing so. The  <code>code-server</code> image used is similar to the normal VSCode, but can be hosted online if you know how.</p>"},{"location":"docker/03b-dev-wksp/#vscode","title":"VSCode","text":""},{"location":"docker/03b-dev-wksp/#spinning-the-docker-container-up","title":"Spinning the Docker container up","text":"<p>The Docker image we are using has special user permissions to UID 2222 and GID 2222. As such, we would create a new folder named <code>workspaces</code> to copy the codebase inside and interact with it. (Only affects Linux)</p> <p>You can spin the Docker container up as such:</p> LinuxmacOSWindows PowerShell <pre><code>docker run -it --name code-server -p 127.0.0.1:8080:8080 \\\n  -v \"$HOME:/home/coder\" \\\n  -u \"$(id -u):$(id -g)\" \\\n  -e \"DOCKER_USER=$USER\" \\\n  asia-southeast1-docker.pkg.dev/machine-learning-ops/pub-images/code-server:latest\n</code></pre> <pre><code>docker run -it --name code-server -p 127.0.0.1:8080:8080 \\\n  -v \"$HOME:/home/coder\" \\\n  -e \"DOCKER_USER=$USER\" \\\n  asia-southeast1-docker.pkg.dev/machine-learning-ops/pub-images/code-server:latest\n</code></pre> <pre><code>docker run -it --name code-server -p 127.0.0.1:8080:8080 `\n  -v \"$Env:USERPROFILE:/home/coder\" `\n  -e \"DOCKER_USER=$Env:USERNAME\" `\n  asia-southeast1-docker.pkg.dev/machine-learning-ops/pub-images/code-server:latest\n</code></pre>"},{"location":"docker/03b-dev-wksp/#extensions-for-vscode","title":"Extensions for VSCode","text":"<p>You can install a multitude of extensions for your VSCode service but there are a couple that would be crucial for your workflow, especially if you intend to use Jupyter notebooks within the VSCode environment.</p> <ul> <li><code>ms-python.python</code>: Official extension by Microsoft for   rich support for many things Python.</li> <li><code>ms-toolsai.jupyter</code>: Official extension by Microsoft    for Jupyter support.</li> </ul> <p>Manual Installation</p> <p>For some clusters, you may need to install the extensions manually due to firewall issues. If that is the case, you can download the extension(s) through your local machine and upload them to the  VSCode terminal. From there, you can make use of the following  command: <pre><code>$ code-server --install-extension /path/to/extension.vsix\n</code></pre></p> <p>Attention</p> <p>Do head over here on how to enable the usage of  virtual <code>conda</code> environments within VSCode.</p>"},{"location":"docker/04b-virtual-env/","title":"Virtual Environments","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Incompatibility issues</p> <p>This method is not recommended as it may have unintended consequences on user and group permissions. It is highly recommended to run locally instead. This section is to provide a complimentary section as a means of a technical possibility, especially should you require debugging within the Docker container.</p> <p>Creating Virtual Environments</p> <p>If you're planning to use the <code>code-server</code> development workspace written in the previous section, you should start reading here  instead.</p>"},{"location":"docker/04b-virtual-env/#docker-image-debugging","title":"Docker Image Debugging","text":"<p>While you might be making use of your own remote infrastructure to carry out some workflows, we can still make use of our local machine to execute some of the steps of the end-to-end machine learning workflow. Hence, we can begin by creating a virtual environment that will contain all the dependencies required for this guide. This requires the Docker image to be built from a Dockerfile (<code>docker/project-cpu.Dockerfile</code>) provided in this template:</p> LinuxmacOSWindows PowerShell <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    -f docker/project-cpu.Dockerfile .\n</code></pre> <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    -f docker/project-cpu.Dockerfile \\\n    --platform linux/amd64 .\n</code></pre> <pre><code>docker build `\n    -t registry.aisingapore.net/project-path/cpu:0.1.0 `\n    -f docker/project-cpu.Dockerfile .\n</code></pre> <p>Using GPUs in Docker</p> <p>You can build the <code>gpu</code> variant by replacing the <code>cpu</code> in the above commands to <code>gpu</code>, i.e.:</p> <ul> <li><code>registry.aisingapore.net/project-path/cpu</code> to   <code>registry.aisingapore.net/project-path/gpu</code></li> <li><code>docker/project-cpu.Dockerfile</code> to   <code>docker/project-gpu.Dockerfile</code></li> </ul> <p>Spin up your Docker image by running:</p> LinuxmacOSWindows PowerShell <p>Info</p> <p>Add <code>--gpus=all</code> for Nvidia GPUs in front of the image name. Add <code>--device=nvidia.com/gpu=all</code> for Nvidia GPUs using Podman instead of Docker. Add <code>--device=/dev/kfd --device=/dev/dri --group-add</code> video for AMD GPUs in front of the image name.</p> <pre><code>docker run -it --rm \\\n    -u $(id -u):$(id -g) \\\n    -v ./:/home/aisg/project \\\n    registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    bash\n</code></pre> <pre><code>docker run -it --rm \\\n    -v ./:/home/aisg/project \\\n    registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    bash\n</code></pre> <p>Warning</p> <p>GPU passthrough only works with Docker Desktop or Podman  Desktop at the time this section is written. For Nvidia GPUs, you would need to add <code>--gpus=all</code> in front of the image name, or <code>--device=nvidia.com/gpu=all</code> if Podman is used. For AMD GPUs, you can follow this guide.</p> <pre><code>docker run -it --rm `\n    -v .\\:/home/aisg/project `\n    registry.aisingapore.net/project-path/cpu:0.1.0 `\n    bash\n</code></pre> <p>You can either run <code>python</code> or install IPython to run an interactive shell and write snippets to test within the environment:</p> <pre><code>pip install ipython\n</code></pre> <p>Why use IPython when there is the Python interpreter?</p> <p>There are a number of things IPython does that the Python  interpreter lacks:</p> <ul> <li>Robust command history with search capabilities, allowing you to   navigate through previously executed commands easily</li> <li>Auto-completion, syntax highlighting and other development tools   that make coding easier and faster</li> <li>Enhanced debugging tools and interactive exception handling</li> <li>Able to use hooks and plugins to enhance the IPython experience</li> </ul>"},{"location":"docker/04b-virtual-env/#using-virtual-conda-environments-within-vscode","title":"Using Virtual Conda Environments Within VSCode","text":"<p>While it is possible for VSCode to make use of different virtual Python environments, some other additional steps are required for the VSCode server to detect the <code>conda</code> environments that you would have created.</p> <ul> <li> <p>Ensure that you are in a project folder which you intend to work   on. You can open a folder through <code>File &gt; Open Folder...</code>.</p> </li> <li> <p>Install the VSCode extensions <code>ms-python.python</code> and   <code>ms-toolsai.jupyter</code>. After installation of these    extensions, restart VSCode. If you wish to restart VSCode in-place,   you can do so by using the shortcut <code>Ctrl + Shift + P</code>, entering    <code>Developer: Reload Window</code> in the prompt and pressing <code>Enter</code>    following that.</p> </li> <li> <p>Ensure that you have <code>ipykernel</code> installed in the <code>conda</code>    environment that you intend to use. This template by default lists    the library as a dependency under  <code>requirements.txt</code>. You can check   for the library like so:</p> Linux/macOSWindows PowerShell <pre><code>conda activate project\nconda list | grep \"ipykernel\"\n</code></pre> <pre><code>conda activate project\nconda list | Select-String \"ipykernel\"\n</code></pre> </li> </ul> <p>Output should look similar to:</p> <pre><code>ipykernel  X.XX.XX  pypi_0  pypi\n</code></pre> <ul> <li> <p>Now enter <code>Ctrl + Shift + P</code> again and execute    <code>Python: Select Interpreter</code>. Provide the path to the Python    executable within the <code>conda</code> environment that you intend to use,    something like so: <code>path/to/conda_env/bin/python</code>.</p> </li> <li> <p>Open up any Jupyter notebook and click on the button that says   <code>Select Kernel</code> on the top right hand corner. You will be presented   with a selection of Python interpreters. Select the one that   corresponds to the environment you intend to use.</p> </li> <li> <p>Test out the kernel by running the cells in the sample notebook   provided under <code>notebooks/sample-pytorch-notebook.ipynb</code>.</p> </li> </ul>"},{"location":"docker/05b-data-storage-versioning/","title":"Data Storage &amp; Versioning","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p>"},{"location":"docker/05b-data-storage-versioning/#sample-data","title":"Sample Data","text":"<p>We can generate some sample data to use to test the different  components of Kapitan Hull.</p> Linux/macOSWindows Powershell <pre><code>mkdir -p ./data/raw &amp;&amp; cd \"$_\"\necho \"Test1\" &gt; data1.txt\necho \"Test2\" &gt; data2.txt\necho \"Test3\" &gt; data3.txt\n</code></pre> <pre><code>New-Item -ItemType Directory -Path .\\data\\raw -Force | Out-Null\nSet-Location -Path .\\data\\raw\nSet-Content -Path .\\data1.txt -Value \"Test1\"\nSet-Content -Path .\\data2.txt -Value \"Test2\"\nSet-Content -Path .\\data3.txt -Value \"Test3\"\n</code></pre> <p>In the next section, we will work towards processing this set of raw data and eventually 'training' a dummy model.</p>"},{"location":"docker/06b-job-orchestration/","title":"Job Orchestration","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>We can set up development workspaces to execute jobs and workflows  locally through Docker containers.</p>"},{"location":"docker/06b-job-orchestration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>In this template, Hydra is the configuration framework of choice for the data preparation and model training pipelines (or any pipelines that doesn't belong to the model serving aspects).</p> <p>The configurations for logging, pipelines and hyperparameter tuning can be found under the <code>conf</code> folder. These YAML files are then referred to  by Hydra or general utility functions (<code>src/project_package/general_utils.py</code>) for loading of parameters and configurations. The defined default  values can be overridden through the CLI.</p> <p>Attention</p> <p>It is recommended that you have a basic understanding of Hydra's concepts before you move on.</p> Reference Link(s) <ul> <li>Hydra Docs - Basic Override Syntax</li> </ul>"},{"location":"docker/06b-job-orchestration/#data-preparation-preprocessing","title":"Data Preparation &amp; Preprocessing","text":"<p>To process the sample raw data, there are many ways to do so. One way is to run through a Docker container. You can first update your configuration variables at <code>conf/process_data.yaml</code>, specifically this section:</p> <pre><code>raw_data_dir_path: \"./data/raw\"\nprocessed_data_dir_path: \"./data/processed\"\nlog_dir: \"./logs\"\n</code></pre> <p>This requires the Docker image to be built from a Dockerfile  (<code>docker/project-cpu.Dockerfile</code>) provided in this template:</p> <p>You may have built the Docker image before</p> <p>If you have followed the section on virtual environments, you would have already built your image. If you have not modified  the configuration variables, you can safely skip this step.</p> LinuxmacOSWindows PowerShell <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    -f docker/project-cpu.Dockerfile .\n</code></pre> <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    -f docker/project-cpu.Dockerfile \\\n    --platform linux/amd64 .\n</code></pre> <pre><code>docker build `\n    -t registry.aisingapore.net/project-path/cpu:0.1.0 `\n    -f docker/project-cpu.Dockerfile .\n</code></pre> <p>After building the image, you can run the script through Docker:</p> LinuxmacOSWindows PowerShell <pre><code>sudo chown 2222:2222 ./data\ndocker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -w /home/aisg/project \\\n    registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    bash -c \"python -u src/process_data.py\"\n</code></pre> <pre><code>docker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -w /home/aisg/project \\\n    registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    bash -c \"python -u src/process_data.py\"\n</code></pre> <pre><code>docker run --rm `\n    -v .\\data:/home/aisg/project/data `\n    -w /home/aisg/project `\n    registry.aisingapore.net/project-path/cpu:0.1.0 `\n    bash -c \"python -u src/process_data.py\"\n</code></pre> <p>Once you are satisfied with the Docker image, you can push it to the  Docker registry:</p> <pre><code>docker push registry.aisingapore.net/project-path/cpu:0.1.0\n</code></pre>"},{"location":"docker/06b-job-orchestration/#model-training","title":"Model Training","text":"<p>Now that we have processed the raw data, we can look into training the model. The script relevant for this section is <code>src/train_model.py</code>. In this script, you can see it using some utility functions from <code>src/project_package/general_utils.py</code> as well, most notably the functions for utilising MLflow utilities for tracking experiments. Let's set up the tooling for experiment tracking before we start model experimentation.</p> <p>Experiment Tracking and Logging</p> <p>In the module <code>src/project_package/general_utils.py</code>, the functions <code>mlflow_init</code> and <code>mlflow_log</code> are used to initialise MLflow experiments as well as log information and artifacts  relevant for a run to an <code>mlruns</code> local folder. After that, we would  use the MLFlow Docker image for analysis.</p> <p>The <code>setup_logging</code> function now supports a <code>log_dir</code> parameter that allows you to specify a custom directory for log files. This is useful when you want to store logs in a specific location, such as a mounted volume in a container environment or a shared directory for team access.</p> Reference Link(s) <ul> <li>MLflow Docs - Tracking</li> </ul> <p>Before building the Docker image to run the model training script, you  should update your configuration variables at <code>conf/train_model.yaml</code>  first, especially this section:</p> <pre><code>setup_mlflow: true\nmlflow_autolog: false\nmlflow_tracking_uri: \"./mlruns\"\nmlflow_exp_name: \"project_package_short\"\nmlflow_run_name: \"train-model\"\ndata_dir_path: \"./data/processed\"\nlr: 1.3\ntrain_bs: 32\ntest_bs: 100\nartifact_dir_path: \"./models\"\nepochs: 5\nresume: false\nlog_dir: \"./logs\"\n</code></pre> <p>After that, we build the Docker image from the Docker file  <code>docker/project-gpu.Dockerfile</code>:</p> <p>You may have built the Docker image before</p> <p>If you have followed the section on virtual environments, you would have already built your image. If you have not modified  the configuration variables, you can safely skip this step.</p> <p>Attention</p> <p>If you're only using CPUs for training, then you can just reuse the <code>project/cpu</code> Docker image instead that was built during the previous step. If you're using AMD GPUs for training, you can copy the components from the <code>rocm</code> folder in the Kapitan Hull repository.</p> LinuxmacOSWindows PowerShell <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    -f docker/project-gpu.Dockerfile .\n</code></pre> <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    -f docker/project-gpu.Dockerfile \\\n    --platform linux/amd64 .\n</code></pre> <pre><code>docker build `\n    -t registry.aisingapore.net/project-path/gpu:0.1.0 `\n    -f docker/project-gpu.Dockerfile .\n</code></pre> <p>Before we run the model training image, you can run MLFlow in Docker as well with the following command:</p> LinuxmacOSWindows PowerShell <pre><code>docker run --rm \\\n    -p 5000:5000 \\\n    -v ./mlruns:/mlruns \\\n    ghcr.io/mlflow/mlflow:v2.20.3 \\\n    mlflow server -h 0.0.0.0\n</code></pre> <pre><code>docker run --rm \\\n    -p 5000:5000 \\\n    -v ./mlruns:/mlruns \\\n    --platform linux/amd64 \\\n    ghcr.io/mlflow/mlflow:v2.20.3 \\\n    mlflow server -h 0.0.0.0\n</code></pre> <pre><code>docker run --rm `\n    -p 5000:5000 `\n    -v .\\mlruns:/mlruns `\n    ghcr.io/mlflow/mlflow:v2.20.3 `\n    mlflow server -h 0.0.0.0\n</code></pre> <p>and connect to http://localhost:5000.</p> <p>Running MLFlow in the background</p> <p>You can run MLFlow in the background by appending <code>-d</code> after <code>docker run</code>, but before the image name. You can also append <code>--restart always</code> so that it runs every time you boot up your machine.</p> <p>After that, you can run the script through Docker while connecting to the running MLFlow server:</p> LinuxmacOSWindows PowerShell <p>Info</p> <p>Add <code>--gpus=all</code> for Nvidia GPUs in front of the image name. Add <code>--device=nvidia.com/gpu=all</code> for Nvidia GPUs using Podman instead of Docker. Add <code>--device=/dev/kfd --device=/dev/dri --group-add</code> video for AMD GPUs in front of the image name.</p> <pre><code>docker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -v ./models:/home/aisg/project/models \\\n    -w /home/aisg/project \\\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 \\\n    --network=host \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    bash -c \"python -u src/train_model.py mlflow_tracking_uri=\\$MLFLOW_TRACKING_URI\"\n</code></pre> <pre><code>docker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -v ./models:/home/aisg/project/models \\\n    -w /home/aisg/project \\\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 \\\n    --network=host \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    bash -c \"python -u src/train_model.py mlflow_tracking_uri=\\$MLFLOW_TRACKING_URI\"\n</code></pre> <p>Warning</p> <p>GPU passthrough only works with Docker Desktop or Podman  Desktop at the time this section is written. For Nvidia GPUs, you would need to add <code>--gpus=all</code> in front of the image name, or <code>--device=nvidia.com/gpu=all</code> if Podman is used. For AMD GPUs, you can follow this guide.</p> <pre><code>docker run --rm \\\n    -v .\\data:/home/aisg/project/data `\n    -v .\\models:/home/aisg/project/models `\n    -w /home/aisg/project `\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 `\n    --network=host `\n    registry.aisingapore.net/project-path/gpu:0.1.0 `\n    bash -c \"python -u src/train_model.py mlflow_tracking_uri=\\$MLFLOW_TRACKING_URI\"\n</code></pre> <p>Once you are satisfied with the Docker image, you can push it to the  Docker registry:</p> <pre><code>docker push registry.aisingapore.net/project-path/gpu:0.1.0\n</code></pre> <p></p> <p>Resuming/adding new epochs</p> <p>The training script supports resuming training from a previous  checkpoint by setting the <code>resume</code> parameter to <code>true</code> in the  configuration file. When this parameter is enabled, the script will:</p> <ol> <li>Check for the latest step logged by MLFlow</li> <li>Offset the epoch step with the latest step from MLFlow</li> <li>Continue training from the last saved epoch</li> <li>Log the continued training as part of the same MLflow run</li> </ol> <p>This is particularly useful when:</p> <ul> <li>You need to extend training for additional epochs after    evaluating initial results</li> <li>Training was interrupted and needs to be continued</li> <li>You want to implement a progressive training strategy with    changing parameters</li> </ul> <p>To use this feature, simply set <code>resume: true</code> in your  <code>conf/train_model.yaml</code> file or append <code>resume=true</code> during runtime  as an override and run the training script as normal.</p>"},{"location":"docker/06b-job-orchestration/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>For many ML problems, we would be bothered with finding the optimal parameters to train our models with. While we are able to override the parameters for our model training workflows, imagine having to sweep through a distribution of values. For example, if you were to seek for the optimal learning rate within a log space, we would have to execute <code>runai submit</code> a myriad of times manually, just to provide the training script with a different learning rate value each time. It is reasonable that one seeks for ways to automate this workflow.</p> <p>Optuna is an optimisation framework designed for ML  use-cases. Its features includes:</p> <ul> <li>ease of modularity,</li> <li>optimisation algorithms for searching the best set of parameters,</li> <li>and parallelisation capabilities for faster sweeps.</li> </ul> <p>In addition, Hydra has a plugin for utilising Optuna which further translates to ease of configuration. To use Hydra's plugin for Optuna, we have to provide further overrides within the YAML config, and this is observed in <code>conf/train_model.yaml</code>:</p> <pre><code>defaults:\n  - override hydra/sweeper: optuna\n  - override hydra/sweeper/sampler: tpe\n\nhydra:\n  sweeper:\n    sampler:\n      seed: 55\n    direction: [\"minimize\", \"maximize\"]\n    study_name: \"base-template\"\n    storage: null\n    n_trials: 3\n    n_jobs: 1\n    params:\n      lr: range(0.9,1.7,step=0.1)\n      train_bs: choice(32,48,64)\n</code></pre> <p>These fields are used by the Optuna Sweeper plugin to configure the Optuna study.</p> <p>Attention</p> <p>The fields defined are terminologies used by Optuna. Therefore, it is recommended that you understand the basics of the tool. This overview video covers well on the concepts  brought upon by Optuna.</p> <p>Here are the definitions for some of the fields:</p> <ul> <li><code>params</code> is used to specify the parameters to be tuned, and the    values to be searched through</li> <li><code>n_trials</code> specifies the number of trials to be executed</li> <li><code>n_jobs</code> specifies the number of trials to be executed in    parallel</li> </ul> <p>As to how the training script would work towards training a model with the best set of parameters, there are two important lines from two different files that we have to pay attention to.</p> <p><code>src/train_model.py</code> <pre><code>...\n    return curr_test_loss, curr_test_accuracy\n...\n</code></pre></p> <p><code>conf/train_model.yaml</code> <pre><code>...\n    direction: [\"minimize\", \"maximize\"]\n...\n</code></pre></p> <p>In the training script the returned variables are to contain values that we seek to optimise for. In this case, we seek to minimise the  loss and maximise the accuracy. The <code>hydra.sweeper.direction</code> field in  the YAML config is used to specify the direction that those variables  are to optimise towards, defined in a positional manner within a list.</p> <p>An additional thing to take note of is that for each trial where a different set of parameters are concerned, a new MLflow run has to be initialised. However, we need to somehow link all these different runs together so that we can compare all the runs within a single Optuna study (set of trials). How we do this is that we provide each trial  with the same tag to be logged to MLflow (<code>hptuning_tag</code>) which would essentially be the date epoch value of the moment you run the Docker  container. This tag is defined using the environment value <code>MLFLOW_HPTUNING_TAG</code>.</p> LinuxmacOSWindows PowerShell <p>Info</p> <p>Add <code>--gpus=all</code> for Nvidia GPUs in front of the image name. Add <code>--device=nvidia.com/gpu=all</code> for Nvidia GPUs using Podman instead of Docker. Add <code>--device=/dev/kfd --device=/dev/dri --group-add</code> video for AMD GPUs in front of the image name.</p> <pre><code>docker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -w /home/aisg/project \\\n    -e MLFLOW_HPTUNING_TAG=$(date +%s) \\\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 \\\n    --network=host \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    bash -c \"python -u src/train_model.py --multirun mlflow_tracking_uri=\\$MLFLOW_TRACKING_URI\"\n</code></pre> <pre><code>docker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -w /home/aisg/project \\\n    -e MLFLOW_HPTUNING_TAG=$(date +%s) \\\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 \\\n    --network=host \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    bash -c \"python -u src/train_model.py --multirun mlflow_tracking_uri=\\$MLFLOW_TRACKING_URI\"\n</code></pre> <p>Warning</p> <p>GPU passthrough only works with Docker Desktop or Podman  Desktop at the time this section is written. For Nvidia GPUs, you would need to add <code>--gpus=all</code> in front of the image name, or <code>--device=nvidia.com/gpu=all</code> if Podman is used. For AMD GPUs, you can follow this guide.</p> <pre><code>$Env:MLFLOW_HPTUNING_TAG = [int][double]::Parse((Get-Date).ToUniversalTime().Subtract([datetime]::UnixEpoch).TotalSeconds)\n[System.Environment]::SetEnvironmentVariable(\"MLFLOW_HPTUNING_TAG\", $Env:MLFLOW_HPTUNING_TAG.ToString())\ndocker run --rm \\\n    -v .\\data:/home/aisg/project/data `\n    -w /home/aisg/project `\n    -e MLFLOW_HPTUNING_TAG=$Env:MLFLOW_HPTUNING_TAG `\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 `\n    --network=host `\n    registry.aisingapore.net/project-path/gpu:0.1.0 `\n    bash -c \"python -u src/train_model.py --multirun mlflow_tracking_uri=\\$MLFLOW_TRACKING_URI\"\n</code></pre> <p></p> Reference Link(s) <ul> <li>Hydra Docs - Optuna Sweeper Plugin</li> <li>MLflow Docs - Search Syntax</li> </ul>"},{"location":"docker/07b-deployment/","title":"Deployment","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Assuming we have a predictive model that we are satisfied with, we can serve it within a REST API service with which requests can be made to and predictions are returned.</p> <p>Python has plenty of web frameworks that we can leverage on to build our REST API. Popular examples include Flask, Django and  Starlette. For this guide however, we will resort to the well-known  FastAPI (which is based on Starlette itself).</p> Reference Link(s) <ul> <li>IBM Technology - What is a REST API? (Video)</li> </ul>"},{"location":"docker/07b-deployment/#model-artifacts","title":"Model Artifacts","text":"<p>Seen in \"Model Training\", we have the trained models saved through the MLflow Tracking server (done through autolog). With that, we have the following pointers to take note of:</p> <ul> <li>By default, each MLflow experiment run is given a unique ID.</li> <li>When artifacts are saved through MLFlow, the artifacts are located   within directories named after the unique IDs of the runs.</li> <li>There are two ways to download the artifacts:<ul> <li>In the directory that MLFlow has been spun up, artifacts are   stored in the <code>mlruns</code> folder. Artifacts for specific runs are   saved to a directory with a convention similar to the following:   <code>&lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts</code>.</li> <li>Alternatively, we can utilise the MLFlow Client library to    retrieve the predictive model. This model can then be propagated    into a mounted volume when we run the Docker image for the REST    APIs. We will be recommending this method in this guide.</li> </ul> </li> </ul>"},{"location":"docker/07b-deployment/#model-serving-fastapi","title":"Model Serving (FastAPI)","text":"<p>FastAPI is a web framework that has garnered much popularity in recent years due to ease of adoption with its comprehensive tutorials, type and schema validation, being async capable and having automated docs, among other things. These factors have made it a popular framework within AI Singapore across many projects.</p> <p>If you were to inspect the <code>src</code> folder, you would notice that there exists more than one package:</p> <ul> <li><code>project_package</code></li> <li><code>project_package_fastapi</code></li> </ul> <p>The former contains the modules for executing pipelines like data  preparation and model training while the latter is dedicated to modules  meant for the REST API. Regardless, the packages can be imported by  each other.</p> <p>Note</p> <p>It is recommended that you grasp some basics of the FastAPI framework, up till the beginner tutorials for better  understanding of this section.</p> <p>Let's try running the boilerplate API server on a local machine. Before doing that, identify from the MLflow dashboard the unique ID of the experiment run that resulted in the predictive model that you would like to serve.</p> <p></p> <p>With reference to the example screenshot above, the UUID for the  experiment run is <code>7251ac3655934299aad4cfebf5ffddbe</code>. Once the ID of  the MLflow run has been obtained, let's download the model that we  intend to serve. Assuming you're in the root of this template's  repository, execute the following commands:</p> LinuxmacOSWindows PowerShell <pre><code>sudo chown 2222:2222 ./models\nexport MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\ndocker run --rm \\\n    -v ./models:/models \\\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 \\\n    --network host \\\n    registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    python -c \"import mlflow; mlflow.artifacts.download_artifacts(artifact_uri='runs:/$MODEL_UUID/', dst_path='/models/$MODEL_UUID')\"\n</code></pre> <pre><code>export MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\ndocker run --rm \\\n    -v ./models:/models \\\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 \\\n    --network host \\\n    registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    python -c \"import mlflow; mlflow.artifacts.download_artifacts(artifact_uri='runs:/$MODEL_UUID/', dst_path='/models/$MODEL_UUID')\"\n</code></pre> <pre><code>$Env:MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\ndocker run --rm `\n    -v .\\models:/models `\n    -e MLFLOW_TRACKING_URI=http://localhost:5000 `\n    --network host `\n    registry.aisingapore.net/project-path/cpu:0.1.0 `\n    python -c \"import mlflow; mlflow.artifacts.download_artifacts(artifact_uri='runs:/$MODEL_UUID/', dst_path='/models/$MODEL_UUID')\"\n</code></pre> <p>Executing the commands above will download the artifacts related to the experiment run <code>&lt;MLFLOW_RUN_UUID&gt;</code> to this repository's subdirectory  <code>models</code>. However, the specific subdirectory that is relevant for our  modules to load will be <code>./models/&lt;MLFLOW_RUN_UUID&gt;/output.txt</code>.</p> <p>Now, let's proceed and spin up an inference server using the package  that exists within the repository.</p>"},{"location":"docker/07b-deployment/#running-the-api-server","title":"Running the API Server","text":"<p>Run the FastAPI server using Gunicorn (for Linux/macOS) or <code>uvicorn</code> (for Windows):</p> <p>Attention</p> <p>Gunicorn is only executable on UNIX-based or UNIX-like systems; this method would not be possible/applicable for Windows machines.</p> LinuxmacOSWindows PowerShell <p>Info</p> <p>Add <code>--gpus=all</code> for Nvidia GPUs in front of the image name. Add <code>--device=nvidia.com/gpu=all</code> for Nvidia GPUs using Podman instead of Docker. Add <code>--device=/dev/kfd --device=/dev/dri --group-add</code> video for AMD GPUs in front of the image name.</p> <pre><code>docker run --rm \\\n    -p 8080:8080 \\\n    -v ./models:/home/aisg/project/models \\\n    -w /home/aisg/project/src \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    gunicorn project_package_fastapi.main:APP \\\n        -k uvicorn.workers.UvicornWorker \\\n        -b 0.0.0.0:8080 -w 2 -t 90\n</code></pre> <pre><code>docker run --rm \\\n    -p 8080:8080 \\\n    -v ./models:/home/aisg/project/models \\\n    -w /home/aisg/project/src \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    gunicorn project_package_fastapi.main:APP \\\n        -k uvicorn.workers.UvicornWorker \\\n        -b 0.0.0.0:8080 -w 2 -t 90\n</code></pre> <p>Warning</p> <p>GPU passthrough only works with Docker Desktop or Podman  Desktop at the time this section is written. For Nvidia GPUs, you would need to add <code>--gpus=all</code> in front of the image name, or <code>--device=nvidia.com/gpu=all</code> if Podman is used. For AMD GPUs, you can follow this guide.</p> <pre><code>docker run --rm `\n    -p 8080:8080 `\n    -v .\\models:/home/aisg/project/models `\n    -w /home/aisg/project/src `\n    registry.aisingapore.net/project-path/gpu:0.1.0 `\n    gunicorn project_package_fastapi.main:APP `\n        -k uvicorn.workers.UvicornWorker `\n        -b 0.0.0.0:8080 -w 2 -t 90\n</code></pre> <p>And with that, our document site for our server is viewable through <code>localhost:8080/docs</code> and will look as such:</p> <p></p> <p>In another terminal, use the <code>curl</code> command to submit a request to the API:</p> LinuxmacOSWindows PowerShell <pre><code>docker run --rm --network=host \\\n    curlimages/curl -X POST \\\n        localhost:8080/api/v1/model/predict \\\n        -H 'Content-Type: application/json' \\\n        -d '\"string\"'\n</code></pre> <pre><code>docker run --rm --network=host \\\n    curlimages/curl -X POST \\\n        localhost:8080/api/v1/model/predict \\\n        -H 'Content-Type: application/json' \\\n        -d '\"string\"'\n</code></pre> <pre><code>docker run --rm --network=host `\n    curlimages/curl -X POST `\n        localhost:8080/api/v1/model/predict `\n        -H 'Content-Type: application/json' `\n        -d '\"string\"'\n</code></pre> <p>Output sample:</p> <pre><code>{\"data\":[{\"input\":\"string\"}]}\n</code></pre> <p>With the returned JSON object, we have successfully submitted a request to the FastAPI server and it returned predictions as part of the response.</p>"},{"location":"docker/07b-deployment/#pydantic-settings","title":"Pydantic Settings","text":"<p>Now you might be wondering, how does the FastAPI server knows the path to the model for it to load? FastAPI utilises Pydantic, a library for  data and schema validation, as well as settings management. There's a  class called <code>Settings</code> under the module <code>src/project_package_fastapi/config.py</code>. This class  contains several fields: some are defined and some others not. The  <code>MODEL_UUID</code> field inherits its value from the environment variables.</p> <p><code>src/project_package_fastapi/config.py</code>: <pre><code>...\nclass Settings(pydantic_settings.BaseSettings):\n\n    API_NAME: str = \"project_package_fastapi\"\n    API_V1_STR: str = \"/api/v1\"\n    LOGGER_CONFIG_PATH: str = \"../conf/logging.yaml\"\n\n    MODEL_UUID: str\n...\n</code></pre></p> <p>FastAPI automatically generates interactive API documentation for easy viewing of all the routers/endpoints you have made available for the server. You can view the documentation through <code>&lt;API_SERVER_URL&gt;:&lt;PORT&gt;/docs</code>. </p> Reference Link(s) <ul> <li>PyTorch Tutorials - Saving and Loading Models</li> <li>FastAPI Docs</li> <li>Pydantic Docs - Settings Management</li> <li><code>curl</code> tutorial</li> </ul>"},{"location":"docker/08b-batch-inferencing/","title":"Batch Inferencing","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Some problem statements do not warrant the deployment of an API server but instead methods for conducting batched inferencing where a batch of data is provided to a script and it is able to churn out a set of predictions, perhaps exported to a file.</p> <p>This template provides a Python script (<code>src/batch_infer.py</code>) and a  configuration file (<code>conf/batch_infer.yaml</code>) for this purpose. </p> <p>Let's first create some sample data on our local machine for us to conduct batch inferencing on:</p> LinuxmacOSWindows PowerShell <pre><code>docker run --rm \\\n    -u $(id -u):$(id -g) \\\n    -w /batch-infer \\\n    -v ./data/batch-infer:/batch-infer \\\n    alpine \\\n        bash -c \"echo -n 'Output1' &gt; in1.txt &amp;&amp; \\\n            echo -n 'Output2' &gt; in2.txt &amp;&amp; \\\n            echo -n 'Output3' &gt; in3.txt\"\n</code></pre> <pre><code>docker run --rm \\\n    -w /batch-infer \\\n    -v ./data/batch-infer:/batch-infer \\\n    alpine \\\n        bash -c \"echo -n 'Output1' &gt; in1.txt &amp;&amp; \\\n            echo -n 'Output2' &gt; in2.txt &amp;&amp; \\\n            echo -n 'Output3' &gt; in3.txt\"\n</code></pre> <pre><code>docker run --rm `\n    -w /batch-infer `\n    -v .\\data\\batch-infer:/batch-infer `\n    alpine `\n        bash -c \"echo -n 'Output1' &gt; in1.txt &amp;&amp; `\n            echo -n 'Output2' &gt; in2.txt &amp;&amp; `\n            echo -n 'Output3' &gt; in3.txt\"\n</code></pre> <p>To execute the batch inferencing script using the Docker image:</p> LinuxmacOSWindows PowerShell <pre><code>docker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -w /home/aisg/project \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    python src/batch_infer.py output_path=data/batch-infer/batch_infer_res.jsonl\nsudo chmod $(id -u):$(id -g) data/batch-infer/batch_infer_res.jsonl\n</code></pre> <pre><code>docker run --rm \\\n    -v ./data:/home/aisg/project/data \\\n    -w /home/aisg/project \\\n    registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    python src/batch_infer.py output_path=data/batch-infer/batch_infer_res.jsonl\n</code></pre> <pre><code>docker run --rm `\n    -v .\\data:/home/aisg/project/data `\n    -w /home/aisg/project `\n    registry.aisingapore.net/project-path/gpu:0.1.0 `\n    python src/batch_infer.py output_path=data/batch-infer/batch_infer_res.jsonl\n</code></pre> <p>The script will log to the terminal the location of the <code>.jsonl</code> file (<code>batch-infer-res.jsonl</code>) containing predictions that look like such:</p> <pre><code>...\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in1.txt\", \"prediction\": \"Output1\"}\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in2.txt\", \"prediction\": \"Output2\"}\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in3.txt\", \"prediction\": \"Output3\"}\n...\n</code></pre> <p>The <code>hydra.job.chdir=True</code> flag writes the <code>.jsonl</code> file containing the predictions to a subdirectory within the <code>outputs</code> folder. See  here for more information on outputs generated by Hydra.</p>"},{"location":"local/03a-dev-wksp/","title":"Development Workspace","text":"<p>There are many local platforms that can be utilised, of which we favour VSCode/VSCodium. In this guide, we would refer both VSCode/VSCodium as VSCode.</p>"},{"location":"local/03a-dev-wksp/#vscode","title":"VSCode","text":""},{"location":"local/03a-dev-wksp/#extensions-for-vscode","title":"Extensions for VSCode","text":"<p>You can install a multitude of extensions for your VSCode service but there are a couple that would be crucial for your workflow, especially if you intend to use Jupyter notebooks within the VSCode environment.</p> <ul> <li><code>ms-python.python</code>: Official extension by Microsoft for   rich support for many things Python.</li> <li><code>ms-toolsai.jupyter</code>: Official extension by Microsoft    for Jupyter support.</li> </ul> <p>Attention</p> <p>Do head over here on how to enable the usage of  virtual <code>conda</code> environments within VSCode.</p>"},{"location":"local/04a-virtual-env/","title":"Virtual Environments","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>We can create a virtual environment that will contain all the  dependencies required for this guide. While you might be making use of your own remote infrastructure to carry out some workflows, we can still make use of our local machine to execute some of the steps of the end-to-end machine learning workflow.</p> <pre><code>conda env create -f project-conda-env.yaml\n</code></pre> <p>The Conda YAML configuration file uses the <code>requirements.txt</code> to create a Conda environment. This is so that there is parity between the  development and deployment environment as the Docker image would use  <code>requirements.txt</code> as the list of packages to be installed.</p> <p>You may have to do extra pip installations, depending on your project's requirements. You can install them through <code>pip install -r &lt;file&gt;</code>.</p> <p>On the use of GPUs</p> <p>Conda environment configured using the YAML file does not take into account whether you need extra requirements to use your GPU for training/inference. Check the instructions on your ML/AI framework of choice to configure your Conda environment to suit your needs.</p> <p>Activate your environment by running:</p> <pre><code>conda activate project\n</code></pre> <p>You can either run <code>python</code> or install IPython to run an interactive shell and write snippets to test within the environment:</p> <pre><code>conda install ipython\n</code></pre> <p>Why use IPython when there is the Python interpreter?</p> <p>There are a number of things IPython does that the Python  interpreter lacks:</p> <ul> <li>Robust command history with search capabilities, allowing you to   navigate through previously executed commands easily</li> <li>Auto-completion, syntax highlighting and other development tools   that make coding easier and faster</li> <li>Enhanced debugging tools and interactive exception handling</li> <li>Able to use hooks and plugins to enhance the IPython experience</li> </ul>"},{"location":"local/04a-virtual-env/#using-virtual-conda-environments-within-vscode","title":"Using Virtual Conda Environments Within VSCode","text":"<p>While it is possible for VSCode to make use of different virtual Python environments, some other additional steps are required for the VSCode server to detect the <code>conda</code> environments that you would have created.</p> <ul> <li> <p>Ensure that you are in a project folder which you intend to work   on. You can open a folder through <code>File &gt; Open Folder...</code>.</p> </li> <li> <p>Install the VSCode extensions <code>ms-python.python</code> and   <code>ms-toolsai.jupyter</code>. After installation of these    extensions, restart VSCode. If you wish to restart VSCode in-place,   you can do so by using the shortcut <code>Ctrl + Shift + P</code>, entering    <code>Developer: Reload Window</code> in the prompt and pressing <code>Enter</code>    following that.</p> </li> <li> <p>Ensure that you have <code>ipykernel</code> installed in the <code>conda</code>    environment that you intend to use. This template by default lists    the library as a dependency under  <code>requirements.txt</code>. You can check   for the library like so:</p> Linux/macOSWindows PowerShell <pre><code>conda activate project\nconda list | grep \"ipykernel\"\n</code></pre> <pre><code>conda activate project\nconda list | Select-String \"ipykernel\"\n</code></pre> </li> </ul> <p>Output should look similar to:</p> <pre><code>ipykernel  X.XX.XX  pypi_0  pypi\n</code></pre> <ul> <li> <p>Now enter <code>Ctrl + Shift + P</code> again and execute    <code>Python: Select Interpreter</code>. Provide the path to the Python    executable within the <code>conda</code> environment that you intend to use,    something like so: <code>path/to/conda_env/bin/python</code>.</p> </li> <li> <p>Open up any Jupyter notebook and click on the button that says   <code>Select Kernel</code> on the top right hand corner. You will be presented   with a selection of Python interpreters. Select the one that   corresponds to the environment you intend to use.</p> </li> <li> <p>Test out the kernel by running the cells in the sample notebook   provided under <code>notebooks/sample-pytorch-notebook.ipynb</code>.</p> </li> </ul>"},{"location":"local/05a-data-storage-versioning/","title":"Data Storage &amp; Versioning","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p>"},{"location":"local/05a-data-storage-versioning/#sample-data","title":"Sample Data","text":"<p>We can generate some sample data to use to test the different  components of Kapitan Hull.</p> Linux/macOSWindows Powershell <pre><code>mkdir -p ./data/raw &amp;&amp; cd \"$_\"\necho \"Test1\" &gt; data1.txt\necho \"Test2\" &gt; data2.txt\necho \"Test3\" &gt; data3.txt\n</code></pre> <pre><code>New-Item -ItemType Directory -Path .\\data\\raw -Force | Out-Null\nSet-Location -Path .\\data\\raw\nSet-Content -Path .\\data1.txt -Value \"Test1\"\nSet-Content -Path .\\data2.txt -Value \"Test2\"\nSet-Content -Path .\\data3.txt -Value \"Test3\"\n</code></pre> <p>In the next section, we will work towards processing this set of raw data and eventually 'training' a dummy model.</p>"},{"location":"local/06a-job-orchestration/","title":"Job Orchestration","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>We can set up development workspaces to execute jobs and workflows  locally.</p>"},{"location":"local/06a-job-orchestration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>In this template, Hydra is the configuration framework of choice for the data preparation and model training pipelines (or any pipelines that doesn't belong to the model serving aspects).</p> <p>The configurations for logging, pipelines and hyperparameter tuning can be found under the <code>conf</code> folder. These YAML files are then referred to  by Hydra or general utility functions (<code>src/project_package/general_utils.py</code>) for loading of parameters and configurations. The defined default  values can be overridden through the CLI.</p> <p>Attention</p> <p>It is recommended that you have a basic understanding of Hydra's concepts before you move on.</p> Reference Link(s) <ul> <li>Hydra Docs - Basic Override Syntax</li> </ul>"},{"location":"local/06a-job-orchestration/#data-preparation-preprocessing","title":"Data Preparation &amp; Preprocessing","text":"<p>To process the sample raw data, there are many ways to do so. One way is to run it locally. Ensure that you have activated your Conda  environment before running the script. More information on this can be found here. You can also update your configuration variables at <code>conf/process_data.yaml</code>, specifically this section:</p> <pre><code>raw_data_dir_path: \"./data/raw\"\nprocessed_data_dir_path: \"./data/processed\"\nlog_dir: \"./logs\"\n</code></pre> <p>After that, run the script:</p> Linux/macOSWindows PowerShell <pre><code># Add no_cuda=False at the end to enable GPU use.\n# Make sure you have installed CUDA/RoCM before using.\n# Check that LD_LIBRARY_PATH has been set.\n# Also set HIP_VISIBLE_DEVICES=0 if RoCM is used.\npython src/process_data.py\n</code></pre> <pre><code>python src\\process_data.py\n</code></pre>"},{"location":"local/06a-job-orchestration/#model-training","title":"Model Training","text":"<p>Now that we have processed the raw data, we can look into 'training'  the model. The script relevant for this section is <code>src/train_model.py</code>. </p> <p>In this script, you can see it using some utility functions from <code>src/project_package/general_utils.py</code> as well, most  notably the functions for utilising MLflow utilities for tracking  experiments. Let's set up the tooling for experiment tracking before we  start model experimentation.</p> <p>Experiment Tracking and Logging</p> <p>In the module <code>src/project_package/general_utils.py</code>, the functions <code>mlflow_init</code> and <code>mlflow_log</code> are used to initialise MLflow experiments as well as log information and artifacts relevant for a run to an <code>mlruns</code> local folder.</p> <p>The <code>setup_logging</code> function now supports a <code>log_dir</code> parameter that allows you to specify a custom directory for log files. This is useful when you want to store logs in a specific location, such as a mounted volume in a container environment or a shared directory for team access.</p> Reference Link(s) <ul> <li>MLflow Docs - Tracking</li> </ul> <p>To run the model training script locally, you should have your Conda  environment activated from the data preparation stage, and update your configuration variables at <code>conf/train_model.yaml</code>, especially this section:</p> <pre><code>setup_mlflow: true\nmlflow_autolog: false\nmlflow_tracking_uri: \"./mlruns\"\nmlflow_exp_name: \"project_package_short\"\nmlflow_run_name: \"train-model\"\ndata_dir_path: \"./data/processed\"\nlr: 1.3\ntrain_bs: 32\ntest_bs: 100\nartifact_dir_path: \"./models\"\nepochs: 5\nresume: false\nlog_dir: \"./logs\"\n</code></pre> <p>After that, run the script:</p> Linux/macOSWindows PowerShell <pre><code>python src/train_model.py\n</code></pre> <pre><code>python src\\train_model.py\n</code></pre> <p>This will generate the MLFlow logs and artifacts locally, of which you  can parse it with the MLFlow UI with:</p> <pre><code>conda create -n mlflow-test mlflow\nconda activate mlflow-test\nmlflow server\n</code></pre> <p>You may not use the <code>project</code> Conda environment</p> <p>You would most likely not able to use the <code>project</code> Conda environment to run <code>mlflow server</code> as the package installed within that environment is <code>mlflow-skinny</code>, not the fully-featured <code>mlflow</code> that contains the server components.</p> <p>and connect to http://localhost:5000.</p> <p></p> <p>Resuming/adding new epochs</p> <p>The training script supports resuming training from a previous  checkpoint by setting the <code>resume</code> parameter to <code>true</code> in the  configuration file. When this parameter is enabled, the script will:</p> <ol> <li>Check for the latest step logged by MLFlow</li> <li>Offset the epoch step with the latest step from MLFlow</li> <li>Continue training from the last saved epoch</li> <li>Log the continued training as part of the same MLflow run</li> </ol> <p>This is particularly useful when:</p> <ul> <li>You need to extend training for additional epochs after    evaluating initial results</li> <li>Training was interrupted and needs to be continued</li> <li>You want to implement a progressive training strategy with    changing parameters</li> </ul> <p>To use this feature, simply set <code>resume: true</code> in your  <code>conf/train_model.yaml</code> file or append <code>resume=true</code> during runtime  as an override and run the training script as normal.</p>"},{"location":"local/06a-job-orchestration/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>For many ML problems, we would be bothered with finding the optimal parameters to train our models with. While we are able to override the parameters for our model training workflows, imagine having to sweep through a distribution of values. For example, if you were to seek for the optimal learning rate within a log space, we would have to execute <code>runai submit</code> a myriad of times manually, just to provide the training script with a different learning rate value each time. It is reasonable that one seeks for ways to automate this workflow.</p> <p>Optuna is an optimisation framework designed for ML  use-cases. Its features includes:</p> <ul> <li>ease of modularity,</li> <li>optimisation algorithms for searching the best set of parameters,</li> <li>and parallelisation capabilities for faster sweeps.</li> </ul> <p>In addition, Hydra has a plugin for utilising Optuna which further translates to ease of configuration. To use Hydra's plugin for Optuna, we have to provide further overrides within the YAML config, and this is observed in <code>conf/train_model.yaml</code>:</p> <pre><code>defaults:\n  - override hydra/sweeper: optuna\n  - override hydra/sweeper/sampler: tpe\n\nhydra:\n  sweeper:\n    sampler:\n      seed: 55\n    direction: [\"minimize\", \"maximize\"]\n    study_name: \"base-template\"\n    storage: null\n    n_trials: 3\n    n_jobs: 1\n    params:\n      lr: range(0.9,1.7,step=0.1)\n      train_bs: choice(32,48,64)\n</code></pre> <p>These fields are used by the Optuna Sweeper plugin to configure the Optuna study.</p> <p>Attention</p> <p>The fields defined are terminologies used by Optuna. Therefore, it is recommended that you understand the basics of the tool. This overview video covers well on the concepts  brought upon by Optuna.</p> <p>Here are the definitions for some of the fields:</p> <ul> <li><code>params</code> is used to specify the parameters to be tuned, and the    values to be searched through</li> <li><code>n_trials</code> specifies the number of trials to be executed</li> <li><code>n_jobs</code> specifies the number of trials to be executed in    parallel</li> </ul> <p>As to how the training script would work towards training a model with the best set of parameters, there are two important lines from two different files that we have to pay attention to.</p> <p><code>src/train_model.py</code> <pre><code>...\n    return curr_test_loss, curr_test_accuracy\n...\n</code></pre></p> <p><code>conf/train_model.yaml</code> <pre><code>...\n    direction: [\"minimize\", \"maximize\"]\n...\n</code></pre></p> <p>In the training script the returned variables are to contain values that we seek to optimise for. In this case, we seek to minimise the  loss and maximise the accuracy. The <code>hydra.sweeper.direction</code> field in  the YAML config is used to specify the direction that those variables  are to optimise towards, defined in a positional manner within a list.</p> <p>An additional thing to take note of is that for each trial where a different set of parameters are concerned, a new MLflow run has to be initialised. However, we need to somehow link all these different runs together so that we can compare all the runs within a single Optuna study (set of trials). How we do this is that we provide each trial  with the same tag to be logged to MLflow (<code>hptuning_tag</code>) which would essentially be the date epoch value of the moment you run the script. This tag is defined using the environment value <code>MLFLOW_HPTUNING_TAG</code>.</p> Linux/macOSWindows PowerShell <pre><code>python src/train_model.py --multirun\n</code></pre> <pre><code>python src\\train_model.py --multirun\n</code></pre> <p></p> Reference Link(s) <ul> <li>Hydra Docs - Optuna Sweeper Plugin</li> <li>MLflow Docs - Search Syntax</li> </ul>"},{"location":"local/07a-deployment/","title":"Deployment","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Assuming we have a predictive model that we are satisfied with, we can serve it within a REST API service with which requests can be made to and predictions are returned.</p> <p>Python has plenty of web frameworks that we can leverage on to build our REST API. Popular examples include Flask, Django and  Starlette. For this guide however, we will resort to the well-known  FastAPI (which is based on Starlette itself).</p> Reference Link(s) <ul> <li>IBM Technology - What is a REST API? (Video)</li> </ul>"},{"location":"local/07a-deployment/#model-artifacts","title":"Model Artifacts","text":"<p>Seen in \"Model Training\", we have the trained models saved through the MLflow Tracking server (done through autolog). With that, we have the following pointers to take note of:</p> <ul> <li>By default, each MLflow experiment run is given a unique ID.</li> <li>When artifacts are saved through MLFlow, the artifacts are located   within directories named after the unique IDs of the runs.</li> <li>There are two ways to download the artifacts:<ul> <li>In the directory that MLFlow has been spun up, artifacts are   stored in the <code>mlruns</code> folder. Artifacts for specific runs are   saved to a directory with a convention similar to the following:   <code>&lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts</code>.</li> <li>Alternatively, we can utilise the MLFlow Client library to    retrieve the predictive model. This model can then be propagated    into a mounted volume when we run the Docker image for the REST    APIs. We will be recommending this method in this guide.</li> </ul> </li> </ul>"},{"location":"local/07a-deployment/#model-serving-fastapi","title":"Model Serving (FastAPI)","text":"<p>FastAPI is a web framework that has garnered much popularity in recent years due to ease of adoption with its comprehensive tutorials, type and schema validation, being async capable and having automated docs, among other things. These factors have made it a popular framework within AI Singapore across many projects.</p> <p>If you were to inspect the <code>src</code> folder, you would notice that there exists more than one package:</p> <ul> <li><code>project_package</code></li> <li><code>project_package_fastapi</code></li> </ul> <p>The former contains the modules for executing pipelines like data  preparation and model training while the latter is dedicated to modules  meant for the REST API. Regardless, the packages can be imported by  each other.</p> <p>Note</p> <p>It is recommended that you grasp some basics of the FastAPI framework, up till the beginner tutorials for better  understanding of this section.</p> <p>Let's try running the boilerplate API server on a local machine. Before doing that, identify from the MLflow dashboard the unique ID of the experiment run that resulted in the predictive model that you would like to serve.</p> <p></p> <p>With reference to the example screenshot above, the UUID for the  experiment run is <code>7251ac3655934299aad4cfebf5ffddbe</code>. Once the ID of  the MLflow run has been obtained, let's download the model that we  intend to serve. Assuming you're in the root of this template's  repository, execute the following commands:</p> Linux/macOSWindows PowerShell <pre><code>conda activate project\nexport MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\npython -c \"import mlflow; mlflow.artifacts.download_artifacts(artifact_uri='runs:/$MODEL_UUID/', dst_path='models/$MODEL_UUID')\"\n</code></pre> <pre><code>conda activate project\n$Env:MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\npython -c \"import mlflow; mlflow.artifacts.download_artifacts(artifact_uri='runs:/$MODEL_UUID/', dst_path='models/$MODEL_UUID')\"\n</code></pre> <p>Executing the commands above will download the artifacts related to the experiment run <code>&lt;MLFLOW_RUN_UUID&gt;</code> to this repository's subdirectory  <code>models</code>. However, the specific subdirectory that is relevant for our  modules to load will be <code>./models/&lt;MLFLOW_RUN_UUID&gt;/output.txt</code>.</p> <p>Now, let's proceed and spin up an inference server using the package  that exists within the repository.</p>"},{"location":"local/07a-deployment/#running-the-api-server","title":"Running the API Server","text":"<p>Run the FastAPI server using Gunicorn (for Linux/macOS) or <code>uvicorn</code> (for Windows):</p> <p>Attention</p> <p>Gunicorn is only executable on UNIX-based or UNIX-like systems; this method would not be possible/applicable for Windows machines.</p> Linux/macOSWindows PowerShell <pre><code>conda activate project\ngunicorn project_package_fastapi.main:APP \\\n    -k uvicorn.workers.UvicornWorker \\\n    -b 0.0.0.0:8080 -w 2 -t 90 --chdir src\n</code></pre> <p>Info</p> <p>See here as to why Gunicorn is to be used instead of just Uvicorn. TLDR: Gunicorn is needed to spin up  multiple processes/workers to handle more requests i.e. better  for the sake of production needs.</p> <pre><code>conda activate project\nuvicorn project_package_fastapi.main:APP --app-dir src\n</code></pre> <p>And with that, our document site for our server is viewable through <code>localhost:8080/docs</code> and will look as such:</p> <p></p> <p>In another terminal, use the <code>curl</code> command to submit a request to the API:</p> Linux/macOSWindows PowerShell <pre><code>curl -X POST \\\n    localhost:8080/api/v1/model/predict \\\n    -H 'Content-Type: application/json' \\\n    -d '\"string\"'\n</code></pre> <pre><code>curl.exe '-X', 'POST', `\n    'localhost:8080/api/v1/model/predict', `\n    '-H', 'Content-Type: application/json', `\n    '-d', '\"string\"',\n</code></pre> <p>Output sample:</p> <pre><code>{\"data\":[{\"input\":\"string\"}]}\n</code></pre> <p>With the returned JSON object, we have successfully submitted a request to the FastAPI server and it returned predictions as part of the response.</p>"},{"location":"local/07a-deployment/#pydantic-settings","title":"Pydantic Settings","text":"<p>Now you might be wondering, how does the FastAPI server knows the path to the model for it to load? FastAPI utilises Pydantic, a library for  data and schema validation, as well as settings management. There's a  class called <code>Settings</code> under the module <code>src/project_package_fastapi/config.py</code>. This class  contains several fields: some are defined and some others not. The  <code>MODEL_UUID</code> field inherits its value from the environment variables.</p> <p><code>src/project_package_fastapi/config.py</code>: <pre><code>...\nclass Settings(pydantic_settings.BaseSettings):\n\n    API_NAME: str = \"project_package_fastapi\"\n    API_V1_STR: str = \"/api/v1\"\n    LOGGER_CONFIG_PATH: str = \"../conf/logging.yaml\"\n\n    MODEL_UUID: str\n...\n</code></pre></p> <p>FastAPI automatically generates interactive API documentation for easy viewing of all the routers/endpoints you have made available for the server. You can view the documentation through <code>&lt;API_SERVER_URL&gt;:&lt;PORT&gt;/docs</code>. </p> Reference Link(s) <ul> <li>PyTorch Tutorials - Saving and Loading Models</li> <li>FastAPI Docs</li> <li>Pydantic Docs - Settings Management</li> <li><code>curl</code> tutorial</li> </ul>"},{"location":"local/08a-batch-inferencing/","title":"Batch Inferencing","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Some problem statements do not warrant the deployment of an API server but instead methods for conducting batched inferencing where a batch of data is provided to a script and it is able to churn out a set of predictions, perhaps exported to a file.</p> <p>This template provides a Python script (<code>src/batch_infer.py</code>) and a  configuration file (<code>conf/batch_infer.yaml</code>) for this purpose. </p> <p>Let's first create some sample data on our local machine for us to  conduct batch inferencing on:</p> Linux/macOSWindows PowerShell <pre><code>mkdir -p data/batch-infer &amp;&amp; cd $_\necho -n \"Output1\" &gt; in1.txt\necho -n \"Output2\" &gt; in2.txt\necho -n \"Output3\" &gt; in3.txt\n</code></pre> <pre><code>New-Item -ItemType Directory -Force -Path 'data/batch-infer'\n$currentDirectory = Get-Location\nSet-Location -Path 'data/batch-infer'\n\nNew-Item -ItemType File -Force -Name 'in1.txt' | Out-Null\nAdd-Content -Path 'in1.txt' -Value \"Output1\"\n\nNew-Item -ItemType File -Force -Name 'in2.txt' | Out-Null\nAdd-Content -Path 'in2.txt' -Value \"Output2\"\n\nNew-Item -ItemType File -Force -Name 'in3.txt' | Out-Null\nAdd-Content -Path 'in3.txt' -Value \"Output3\"\n\nSet-Location -Path $currentDirectory\n</code></pre> <p>To execute the batch inferencing script locally:</p> Linux/macOSWindows PowerShell <pre><code># Navigate back to root directory\ncd \"$(git rev-parse --show-toplevel)\"\nconda activate project\npython src/batch_infer.py\n</code></pre> <pre><code># Navigate back to root directory\nSet-Location -Path (git rev-parse --show-toplevel)\nconda activate project\npython src/batch_infer.py\n</code></pre> <p>The script will log to the terminal the location of the <code>.jsonl</code> file (<code>batch-infer-res.jsonl</code>) containing predictions that look like such:</p> <pre><code>...\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in1.txt\", \"prediction\": \"Output1\"}\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in2.txt\", \"prediction\": \"Output2\"}\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in3.txt\", \"prediction\": \"Output3\"}\n...\n</code></pre> <p>The <code>hydra.job.chdir=True</code> flag writes the <code>.jsonl</code> file containing the predictions to a subdirectory within the <code>outputs</code> folder. See  here for more information on outputs generated by Hydra.</p>"},{"location":"runai/03c-dev-wksp/","title":"Development Workspace","text":"<p>An advantage presented by orchestration platforms is that you can utilise the Kubernetes cluster's resources for your development and engineering works instead of your own resources.</p> <p>We can make use of Coder to spin up VSCode servers with which  cluster resources can be dedicated.</p>"},{"location":"runai/03c-dev-wksp/#coder-vscode-server","title":"Coder &amp; VSCode Server","text":""},{"location":"runai/03c-dev-wksp/#prebuilt-vscode-server-in-coder","title":"Prebuilt VSCode Server in Coder","text":"<p>The MLOps team should have spun up a Coder instance in the cluster and  handed the URL to you. The only thing you would need to do is to log  into Coder with OpenID Connect:</p> <p></p> <p>Once you're in, you should be seeing something similar to this:</p> <p></p> <p>If you do not see a workspace running, or you could not access the  Coder workspace allocated to you, you can contact the MLOps team.</p> <p>If you have the permissions, you could also create a workspace on your  own. The template you have access to will only work for one workspace  at a time, so you could switch out workspaces with different CPU and  RAM resources, depending on the needs of your team's project  requirements.</p> <p>Recommended settings for your first time are as follows:</p> <ul> <li>Workspace Name: <code>&lt;YOUR_HYPHENATED_NAME&gt;-vscode</code></li> <li>CPU: 2 Cores</li> <li>Memory: 4GB</li> </ul> <p></p> <p>If all of it runs normally, you should have two buttons: VS Code  Desktop and code-server. Click on the latter to start running the  remote VSCode Workspace.</p> <p></p> <p>You should be directed to the VSCode server welcome tab without  password prompt.</p> <p></p>"},{"location":"runai/03c-dev-wksp/#persistent-workspaces","title":"Persistent Workspaces","text":"<p>As mentioned, a PVC should be attached to the workspaces to persist changes to the filesystems. You can use the following command to search for the PVC:</p> Coder Workspace Terminal <pre><code>ls -la / | grep \"pvc\"\n</code></pre> Reference Link(s) <ul> <li>Kubernetes Docs - Persistent Volumes</li> </ul> <p>If there's no result, check with the MLOps team about this.</p> <p>By default, the PVCs would contain a <code>workspaces</code> directory with which you can create a subdirectory for yourself treat it as your own  personal workspace, where all your work and other relevant assets can  be persisted.</p> Coder Workspace Terminal <pre><code>cd /&lt;PVC_LOCATION&gt;/workspaces\nmkdir &lt;YOUR_HYPHENATED_NAME&gt;\n</code></pre>"},{"location":"runai/03c-dev-wksp/#git-from-vscode","title":"Git from VSCode","text":"<p>Git by default is installed in the VSCode server image. One thing to take note is that as the persistent storage would be accessible by the rest of your project team members, you should only use the <code>HTTPS</code> protocol to clone the repository as opposed to creating and using an <code>SSH</code> key within the VSCode server.</p> <p>Now, let's clone your repository from the remote:</p> Coder Workspace Terminal <pre><code>cd /&lt;PVC_LOCATION&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;\ngit clone &lt;REMOTE_URL_HTTPS&gt;\ncd ghpages\n</code></pre>"},{"location":"runai/03c-dev-wksp/#extensions-for-vscode","title":"Extensions for VSCode","text":"<p>You can install a multitude of extensions for your VSCode service but there are a couple that would be crucial for your workflow, especially if you intend to use Jupyter notebooks within the VSCode environment.</p> <ul> <li><code>ms-python.python</code>: Official extension by Microsoft for   rich support for many things Python.</li> <li><code>ms-toolsai.jupyter</code>: Official extension by Microsoft    for Jupyter support.</li> </ul> <p>Manual Installation</p> <p>For some clusters, you may need to install the extensions manually due to firewall issues. If that is the case, you can download the extension(s) through your local machine and upload them to the  VSCode terminal. From there, you can make use of the following  command: <pre><code>$ code-server --install-extension /path/to/extension.vsix\n</code></pre></p> <p>Attention</p> <p>Do head over here on how to enable the usage of  virtual <code>conda</code> environments within VSCode.</p>"},{"location":"runai/04c-virtual-env/","title":"Virtual Environments","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>While the Docker images you will be using to run experiments on Run:ai would contain the <code>conda</code> environments you would need, you can also create these virtual environments within your development environment, and have it be persisted. The following set of commands allows you to create the <code>conda</code> environment and store the packages within your own workspace directory:</p> <ul> <li> <p>First, have VSCode open the repository that you have cloned   previously by heading over to the top left hand corner, selecting   <code>File &gt; Open Folder...</code>, and entering the path to the repository.   In this case, you should be navigating to the folder   <code>/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project</code>.</p> </li> <li> <p>Now, let's initialise <code>conda</code> for the bash shell, and create   the virtual environment specified in   <code>project-conda-env.yaml</code>.</p> </li> </ul> Coder Workspace Terminal <pre><code># Usually this is fine\nconda env create -f project-conda-env.yaml\n# However, if your configuration doesn't point towards the PVC source, you can use this instead\n# Consult the MLOps team if you're unsure.\nconda env create \\\n    -f project-conda-env.yaml \\\n    -p /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/conda_envs/project\n</code></pre> If you're using the 2<sup>nd</sup> <code>conda env create</code> option <p>After creating the <code>conda</code> environment, you can create a permanent  alias for easy activation.</p> Coder Workspace Terminal <pre><code>echo 'alias project-conda=\"conda activate /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/conda_envs/project\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nproject-conda\n# conda environment has been activated as project\n</code></pre> <p>Tip</p> <p>If you encounter issues in trying to install Python libraries, do ensure that the amount of resources allocated to the VSCode server is sufficient. Installation of libraries from PyPI tends to fail when there's insufficient memory. For starters, dedicate 4GB of memory to the service:</p> <p>Another way is to add the flag <code>--no-cache-dir</code> for your <code>pip install</code> executions. However, there's no similar flag for <code>conda</code> at the moment so the above is a blanket solution.</p> <p>The Conda YAML configuration file uses the <code>requirements.txt</code> to create a Conda environment. This is so that there is parity between the  development and deployment environment as the Docker image would use  <code>requirements.txt</code> as the list of packages to be installed.</p> <p>You may have to do extra pip installations, depending on your project's requirements. You can install them through <code>pip install -r &lt;file&gt;</code>.</p> Reference Link(s) <ul> <li><code>conda</code> Docs - Managing environments</li> <li>StackOverflow - \"Pip install killed - out of memory - how to get around it?\"</li> <li>phoenixNAP - Linux alias Command: How to Use It With Examples</li> </ul>"},{"location":"runai/04c-virtual-env/#jupyter-kernel-for-vscode","title":"Jupyter Kernel for VSCode","text":"<p>While it is possible for VSCode to make use of different virtual Python environments, some other additional steps are required for the VSCode server to detect the <code>conda</code> environments that you would have created.</p> <ul> <li> <p>Ensure that you are in a project folder which you intend to work   on. You can open a folder through <code>File &gt; Open Folder...</code>.   In this case, you should be navigating to the folder   <code>/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project</code>.</p> </li> <li> <p>Install the VSCode extensions <code>ms-python.python</code> and   <code>ms-toolsai.jupyter</code>. After installation of these    extensions, restart VSCode by using the shortcut <code>Ctrl + Shift + P</code>,    entering <code>Developer: Reload Window</code> in the prompt and pressing    <code>Enter</code> following that.</p> </li> </ul> <p>Manual Installation</p> <p>For some clusters, you may need to install the extensions manually due to firewall issues. If that is the case, you can download the extension(s) through your local machine and upload them to the  VSCode terminal. From there, you can make use of the following  command: <pre><code>$ code-server --install-extension /path/to/extension.vsix\n</code></pre></p> <ul> <li>Ensure that you have <code>ipykernel</code> installed in the <code>conda</code>    environment that you intend to use. This template by default lists    the library as a dependency under <code>requirements.txt</code>. You can check   for the library like so:</li> </ul> Coder Workspace Terminal <pre><code># Usually this is fine\nconda activate project\n# If you're using the 2nd option\nconda activate /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/conda_envs/project\nconda list | grep \"ipykernel\"\n</code></pre> <p>Output should look similar to:</p> <pre><code>ipykernel  X.XX.XX  pypi_0  pypi\n</code></pre> <ul> <li> <p>Now enter <code>Ctrl + Shift + P</code> again and execute    <code>Python: Select Interpreter</code>. Provide the path to the Python    executable within the <code>conda</code> environment that you intend to use,    something like so: <code>path/to/conda_env/bin/python</code>.</p> </li> <li> <p>Open up any Jupyter notebook and click on the button that says   <code>Select Kernel</code> on the top right hand corner. You will be presented   with a selection of Python interpreters. Select the one that   corresponds to the environment you intend to use.</p> </li> <li> <p>Test out the kernel by running the cells in the sample notebook   provided under <code>notebooks/sample-pytorch-notebook.ipynb</code>.</p> </li> </ul>"},{"location":"runai/05c-data-storage-versioning/","title":"Data Storage &amp; Versioning","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p>"},{"location":"runai/05c-data-storage-versioning/#sample-data","title":"Sample Data","text":"<p>We can generate some sample data to use to test the different  components of Kapitan Hull.</p> Coder Workspace Terminal <pre><code>mkdir -p /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/raw &amp;&amp; cd \"$_\"\necho \"Test1\" &gt; data1.txt\necho \"Test2\" &gt; data2.txt\necho \"Test3\" &gt; data3.txt\n</code></pre> <p>In the next section, we will work towards processing this set of raw data and eventually 'training' a dummy model.</p>"},{"location":"runai/06c-job-orchestration/","title":"Job Orchestration","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Even though we can set up development workspaces to execute jobs and workflows, these environments often have limited access to resources. To carry out heavier workloads, we encourage the usage of job orchestration features that Run:ai offers.</p> <p>Jobs are submitted to the Kubernetes cluster through Run:ai and executed within Docker containers. Using the images specified upon job submission, Kubernetes pods are spun up to execute the entry points or commands defined, tapping on to the cluster's available resources.</p> <p>Any jobs that are submitted to Run:ai can be tracked and monitored through Run:ai's dashboard.</p>"},{"location":"runai/06c-job-orchestration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>In this template, Hydra is the configuration framework of choice for the data preparation and model training pipelines (or any pipelines that doesn't belong to the model serving aspects).</p> <p>The configurations for logging, pipelines and hyperparameter tuning can be found under the <code>conf</code> folder. These YAML files are then referred to  by Hydra or general utility functions (<code>src/package/general_utils.py</code>) for loading of parameters and configurations. The defined default  values can be overridden through the CLI.</p> <p>Attention</p> <p>It is recommended that you have a basic understanding of Hydra's concepts before you move on.</p> Reference Link(s) <ul> <li>Hydra Docs - Basic Override Syntax</li> </ul>"},{"location":"runai/06c-job-orchestration/#data-preparation-preprocessing","title":"Data Preparation &amp; Preprocessing","text":"<p>To process the sample raw data, there are many ways to do so. We can  either build within the Coder workspace, or to submit the job through  Run:ai. You can first update your configuration variables at  <code>conf/process_data.yaml</code>, specifically this section:</p> <pre><code>raw_data_dir_path: \"./data/raw\"\nprocessed_data_dir_path: \"./data/processed\"\nlog_dir: \"./logs\"\n</code></pre> <p>This requires the Docker image to be built from a Dockerfile  (<code>docker/package-cpu.Dockerfile</code>) provided in this template:</p> Coder Workspace TerminalUsing Run:ai <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    -f $(pwd)/docker/project-cpu.Dockerfile \\\n    $(pwd)\n# Run `gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS` \n# and `gcloud auth configure-docker &lt;your-gar-domain-name&gt;`\n# or\n# Run `docker login registry.aisingapore.net`\n# to authenticate if you have not done so\ndocker push registry.aisingapore.net/project-path/cpu:0.1.0\n</code></pre> <pre><code># Run `runai login` and `runai config project project` first if needed\n# Run this in the base of your project repository, and change accordingly\n# Replace --cred-file &lt;file&gt; with --gcp to push to GAR registries\nkhull kaniko --context $(pwd) \\\n    --dockerfile $(pwd)/docker/project-cpu.Dockerfile \\\n    --destination registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    --cred-file /path/to/docker/config.json \\\n    -v &lt;pvc-name&gt;:/path/to/pvc/mount\n</code></pre> <p>Now that we have the Docker image built and pushed to the registry, we  can submit a job using that image to Run:ai:</p> Coder Workspace Terminal using Run:ai <pre><code># Run `runai login` and `runai config project project` first if needed\n# Run this in the base of your project repository, and change accordingly\n# Switch working-dir to /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project to use the repo in the PVC\nrunai submit \\\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-data-prep \\\n    -i registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    --working-dir /home/aisg/project \\\n    --existing-pvc claimname=&lt;NAME_OF_DATA_SOURCE&gt;,path=/&lt;NAME_OF_DATA_SOURCE&gt; \\\n    --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \\\n    --command -- /bin/bash -c \"python -u src/process_data.py \\\n        raw_data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/raw \\\n        processed_data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed \\\n        log_dir=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/logs\"\n</code></pre> <p>After some time, the data processing job should conclude and we can proceed with training the predictive model. The processed data is exported to the directory <code>/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed</code>. We will be passing this path to the model training workflows.</p>"},{"location":"runai/06c-job-orchestration/#model-training","title":"Model Training","text":"<p>Now that we have processed the raw data, we can look into training the sentiment classification model. The script relevant for this section is <code>src/train_model.py</code>. In this script, you can see it using some utility functions from <code>src/project_package/general_utils.py</code> as well, most notably the functions for utilising MLflow utilities for tracking experiments. Let's set up the tooling for experiment tracking before we start model experimentation.</p> <p>Experiment Tracking and Logging</p> <p>In the module <code>src/project_package/general_utils.py</code>, the functions <code>mlflow_init</code> and <code>mlflow_log</code> are used to initialise MLflow experiments as well as log information and artifacts  relevant for a run to a remote MLflow Tracking server. An MLflow  Tracking server is usually set up within the Run:ai project's  namespace for projects that requires model experimentation.  Artifacts logged through the MLflow API can be uploaded to GCS/ECS  buckets, assuming the client is authorised for access to GCS/ECS.</p> <p>The <code>setup_logging</code> function now supports a <code>log_dir</code> parameter that allows you to specify a custom directory for log files. This is useful when you want to store logs in a specific location, such as a mounted volume in a container environment or a shared directory for team access. When running in Run:ai, you might want to set this to a path on your persistent volume.</p> <p>To log and upload artifacts to GCS/ECS buckets through MLFlow,  you need to ensure that the client has access to the credentials of an account that can write to a bucket. This is usually settled by  the MLOps team, so you need only interact with MLFlow to download  the artifacts without explicitly knowing the GCS/ECS credentials.</p> Reference Link(s) <ul> <li>MLflow Docs - Tracking</li> <li>MLflow Docs - Tracking (Artifact Stores)</li> </ul> <p>Before submitting the job to build the Docker image and run the model  training script, you should update your configuration variables at  <code>conf/train_model.yaml</code> first, especially this section:</p> <pre><code>setup_mlflow: true\nmlflow_autolog: false\nmlflow_tracking_uri: \"./mlruns\"\nmlflow_exp_name: \"project_package_short\"\nmlflow_run_name: \"train-model\"\ndata_dir_path: \"./data/processed\"\nlr: 1.3\ntrain_bs: 32\ntest_bs: 100\nartifact_dir_path: \"./models\"\nepochs: 5\nresume: false\nlog_dir: \"./logs\"\n</code></pre> <p>After that, we build the Docker image from the Docker file  <code>docker/project-gpu.Dockerfile</code>:</p> <p>Attention</p> <p>If you're only using CPUs for training, then you can just reuse the <code>project/cpu</code> Docker image instead that was built during the previous step.  </p> Coder Workspace TerminalUsing Run:ai <pre><code>docker build \\\n    -t registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    -f $(pwd)/docker/project-gpu.Dockerfile \\\n    $(pwd)\n# Run `gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS` \n# and `gcloud auth configure-docker &lt;your-gar-domain-name&gt;`\n# or\n# Run `docker login registry.aisingapore.net`\n# to authenticate if you have not done so\ndocker push registry.aisingapore.net/project-path/gpu:0.1.0\n</code></pre> <pre><code># Run `runai login` and `runai config project project` first if needed\n# Run this in the base of your project repository, and change accordingly\n# Replace --cred-file &lt;file&gt; with --gcp to push to GAR registries\nkhull kaniko --context $(pwd) \\\n    --dockerfile $(pwd)/docker/project-gpu.Dockerfile \\\n    --destination registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    --cred-file /path/to/docker/config.json \\\n    -v &lt;pvc-name&gt;:/path/to/pvc/mount\n</code></pre> <p>Now that we have the Docker image built and pushed to the registry,  we can run a job using it:</p> Coder Workspace Terminal using Run:ai <pre><code># Run `runai login` and `runai config project project` first if needed\n# Run this in the base of your project repository, and change accordingly\n# Switch working-dir to /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project to use the repo in the PVC\n$ runai submit \\\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-train \\\n    -i registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    --working-dir /home/aisg/project \\\n    --existing-pvc claimname=&lt;NAME_OF_DATA_SOURCE&gt;,path=/&lt;NAME_OF_DATA_SOURCE&gt; \\\n    --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \\\n    -e MLFLOW_TRACKING_USERNAME=&lt;YOUR_MLFLOW_USERNAME&gt; \\\n    -e MLFLOW_TRACKING_PASSWORD=&lt;YOUR_MLFLOW_PASSWORD&gt; \\\n    --command -- /bin/bash -c \"python -u src/train_model.py \\\n        mlflow_tracking_uri=&lt;MLFLOW_TRACKING_URI&gt; \\\n        mlflow_exp_name=&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt; \\\n        data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed \\\n        artifact_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/models \\\n        log_dir=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/logs\"\n</code></pre> <p>Once you have successfully run an experiment, you may inspect the run on the MLflow Tracking server. Through the MLflow Tracking server interface, you can view the metrics and parameters logged for the run, as well as download the artifacts that have been uploaded to the GCS/ECS bucket. You can also compare runs with each other.</p> <p></p> <p>Tip</p> <p>Every job submitted with <code>runai submit</code> is assigned a unique ID, and a unique job name if the <code>--job-name-prefix</code> is used. The <code>mlflow_init</code> function within the <code>general_utils.py</code> module tags every experiment name with the job's name and UUID as provided by Run:ai, with the tags <code>job_uuid</code> and <code>job_name</code>. This allows you to easily identify the MLflow experiment runs that are associated with  each Run:ai job. You can filter for MLflow experiment runs  associated with a specific Run:ai job by using MLflow's search  filter expressions and API.</p> Reference Link(s) <ul> <li>Run:ai Docs - Environment Variables inside a Run:ai Workload</li> <li>MLflow Docs - Search Runs</li> </ul> <p>Info</p> <p>If your project has GPU quotas assigned to it, you can make use of it by specifying the <code>--gpu</code> flag in the <code>runai submit</code> command. As part of Run:ai's unique selling point, you can also specify fractional values, which would allow you to utilise a fraction of a GPU. This is useful for projects that require a GPU for training, but do not require the full capacity of a GPU.</p> <p>Resuming/adding new epochs</p> <p>The training script supports resuming training from a previous  checkpoint by setting the <code>resume</code> parameter to <code>true</code> in the  configuration file. When this parameter is enabled, the script will:</p> <ol> <li>Check for the latest step logged by MLFlow</li> <li>Offset the epoch step with the latest step from MLFlow</li> <li>Continue training from the last saved epoch</li> <li>Log the continued training as part of the same MLflow run</li> </ol> <p>This is particularly useful when:</p> <ul> <li>You need to extend training for additional epochs after    evaluating initial results</li> <li>Training was interrupted and needs to be continued</li> <li>You want to implement a progressive training strategy with    changing parameters</li> </ul> <p>To use this feature, simply set <code>resume: true</code> in your  <code>conf/train_model.yaml</code> file or append <code>resume=true</code> during runtime  as an override and run the training script as normal.</p>"},{"location":"runai/06c-job-orchestration/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>For many ML problems, we would be bothered with finding the optimal parameters to train our models with. While we are able to override the parameters for our model training workflows, imagine having to sweep through a distribution of values. For example, if you were to seek for the optimal learning rate within a log space, we would have to execute <code>runai submit</code> a myriad of times manually, just to provide the training script with a different learning rate value each time. It is reasonable that one seeks for ways to automate this workflow.</p> <p>Optuna is an optimisation framework designed for ML  use-cases. Its features includes:</p> <ul> <li>ease of modularity,</li> <li>optimisation algorithms for searching the best set of parameters,</li> <li>and parallelisation capabilities for faster sweeps.</li> </ul> <p>In addition, Hydra has a plugin for utilising Optuna which further translates to ease of configuration. To use Hydra's plugin for Optuna, we have to provide further overrides within the YAML config, and this is observed in <code>conf/train_model.yaml</code>:</p> <pre><code>defaults:\n  - override hydra/sweeper: optuna\n  - override hydra/sweeper/sampler: tpe\n\nhydra:\n  sweeper:\n    sampler:\n      seed: 55\n    direction: [\"minimize\", \"maximize\"]\n    study_name: \"base-template\"\n    storage: null\n    n_trials: 3\n    n_jobs: 1\n    params:\n      lr: range(0.9,1.7,step=0.1)\n      train_bs: choice(32,48,64)\n</code></pre> <p>These fields are used by the Optuna Sweeper plugin to configure the Optuna study.</p> <p>Attention</p> <p>The fields defined are terminologies used by Optuna. Therefore, it is recommended that you understand the basics of the tool. This overview video covers well on the concepts  brought upon by Optuna.</p> <p>Here are the definitions for some of the fields:</p> <ul> <li><code>params</code> is used to specify the parameters to be tuned, and the    values to be searched through</li> <li><code>n_trials</code> specifies the number of trials to be executed</li> <li><code>n_jobs</code> specifies the number of trials to be executed in    parallel</li> </ul> <p>As to how the training script would work towards training a model with the best set of parameters, there are two important lines from two different files that we have to pay attention to.</p> <p><code>src/train_model.py</code> <pre><code>...\n    return curr_test_loss, curr_test_accuracy\n...\n</code></pre></p> <p><code>conf/train_model.yaml</code> <pre><code>...\n    direction: [\"minimize\", \"maximize\"]\n...\n</code></pre></p> <p>In the training script the returned variables are to contain values that we seek to optimise for. In this case, we seek to minimise the  loss and maximise the accuracy. The <code>hydra.sweeper.direction</code> field in  the YAML config is used to specify the direction that those variables  are to optimise towards, defined in a positional manner within a list.</p> <p>An additional thing to take note of is that for each trial where a different set of parameters are concerned, a new MLflow run has to be initialised. However, we need to somehow link all these different runs together so that we can compare all the runs within a single Optuna study (set of trials). How we do this is that we provide each trial  with the same tag to be logged to MLflow (<code>hptuning_tag</code>) which would essentially be the date epoch value of the moment you submitted the job to Run:ai. This tag is defined using the environment value <code>MLFLOW_HPTUNING_TAG</code>. This tag is especially useful if you are executing the model training job out of the Run:ai platform, as the <code>JOB_NAME</code> and <code>JOB_UUID</code> environment variables would not be available by default.</p> Coder Workspace Terminal using Run:ai <pre><code># Run `runai login` and `runai config project project` first if needed\n# Run this in the base of your project repository, and change accordingly\n# Switch working-dir to /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project to use the repo in the PVC\nrunai submit \\\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-train-hp \\\n    -i registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    --working-dir /home/aisg/project \\\n    --existing-pvc claimname=&lt;NAME_OF_DATA_SOURCE&gt;,path=/&lt;NAME_OF_DATA_SOURCE&gt; \\\n    --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \\\n    -e MLFLOW_TRACKING_USERNAME=&lt;YOUR_MLFLOW_USERNAME&gt; \\\n    -e MLFLOW_TRACKING_PASSWORD=&lt;YOUR_MLFLOW_PASSWORD&gt; \\\n    -e MLFLOW_HPTUNING_TAG=$(date +%s) \\\n    --command -- /bin/bash -c \"python -u src/train_model.py --multirun \\\n        mlflow_tracking_uri=&lt;MLFLOW_TRACKING_URI&gt; \\\n        mlflow_exp_name=&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt; \\\n        data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed \\\n        artifact_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/models \\\n        log_dir=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/logs\"\n</code></pre> <p></p> Reference Link(s) <ul> <li>Run:ai Docs - Environment Variables inside a Run:ai Workload</li> <li>Hydra Docs - Optuna Sweeper Plugin</li> <li>MLflow Docs - Search Syntax</li> </ul>"},{"location":"runai/07c-deployment/","title":"Deployment","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Assuming we have a predictive model that we are satisfied with, we can serve it within a REST API service with which requests can be made to and predictions are returned.</p> <p>Python has plenty of web frameworks that we can leverage on to build our REST API. Popular examples include Flask, Django and  Starlette. For this guide however, we will resort to the well-known  FastAPI (which is based on Starlette itself).</p> Reference Link(s) <ul> <li>IBM Technology - What is a REST API? (Video)</li> </ul>"},{"location":"runai/07c-deployment/#model-artifacts","title":"Model Artifacts","text":"<p>Seen in \"Model Training\", we have the trained models uploaded to GCS/ECS through the MLflow Tracking server (done through  autolog). With that, we have the following pointers to take note of:</p> <ul> <li>By default, each MLflow experiment run is given a unique ID.</li> <li>When artifacts are uploaded to GCS/ECS through MLflow, the    artifacts are located within directories named after the unique IDs    of the runs.</li> <li>There are two ways to download the artifacts:<ul> <li>We can use the gCloud/AWS CLI to download the predictive model from    GCS/ECS. Artifacts for specific runs will be uploaded to a    directory with a convention similar to the following:   <code>&lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts</code>.</li> <li>Alternatively, we can utilise the MLFlow Client library to    retrieve the predictive model. This model can then be propagated    into a mounted volume when we run the Docker image for the REST    APIs. We will be recommending this method in this guide.</li> </ul> </li> </ul>"},{"location":"runai/07c-deployment/#model-serving-fastapi","title":"Model Serving (FastAPI)","text":"<p>FastAPI is a web framework that has garnered much popularity in recent years due to ease of adoption with its comprehensive tutorials, type and schema validation, being async capable and having automated docs, among other things. These factors have made it a popular framework within AI Singapore across many projects.</p> <p>If you were to inspect the <code>src</code> folder, you would notice that there exists more than one package:</p> <ul> <li><code>project_package</code></li> <li><code>project_package_fastapi</code></li> </ul> <p>The former contains the modules for executing pipelines like data  preparation and model training while the latter is dedicated to modules  meant for the REST API. Regardless, the packages can be imported by  each other.</p> <p>Note</p> <p>It is recommended that you grasp some basics of the FastAPI framework, up till the beginner tutorials for better  understanding of this section.</p> <p>Let's try running the boilerplate API server on a local machine. Before doing that, identify from the MLflow dashboard the unique ID of the experiment run that resulted in the predictive model that you would like to serve.</p> <p></p> <p>With reference to the example screenshot above, the UUID for the  experiment run is <code>7251ac3655934299aad4cfebf5ffddbe</code>. Once the ID of  the MLflow run has been obtained, let's download the model that we  intend to serve. Assuming you're in the root of this template's  repository, execute the following commands:</p> Coder Workspace TerminalUsing Run:ai <pre><code>conda activate project\nexport MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\nexport MLFLOW_TRACKING_URI=&lt;MLFLOW_TRACKING_URI&gt;\nexport MLFLOW_TRACKING_USERNAME=&lt;MLFLOW_TRACKING_USERNAME&gt; # If applicable\nexport MLFLOW_TRACKING_PASSWORD=&lt;MLFLOW_TRACKING_PASSWORD&gt; # If applicable\npython -c \"import mlflow; mlflow.artifacts.download_artifacts(artifact_uri='runs:/$MODEL_UUID/', dst_path='models/$MODEL_UUID')\"\n</code></pre> <pre><code>export MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\nrunai submit \\\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-download-artifacts \\\n    -i registry.aisingapore.net/project-path/cpu:0.1.0 \\\n    --working-dir /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPENATED_NAME&gt; \\\n    --existing-pvc claimname=&lt;NAME_OF_DATA_SOURCE&gt;,path=/&lt;NAME_OF_DATA_SOURCE&gt; \\\n    --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \\\n    -e MLFLOW_TRACKING_URI=&lt;MLFLOW_TRACKING_URI&gt; \\\n    -e MLFLOW_TRACKING_USERNAME=&lt;YOUR_MLFLOW_USERNAME&gt; \\\n    -e MLFLOW_TRACKING_PASSWORD=&lt;YOUR_MLFLOW_PASSWORD&gt; \\\n    --command -- python -c \"import mlflow; mlflow.artifacts.download_artifacts(artifact_uri='runs:/$MODEL_UUID/', dst_path='models/$MODEL_UUID')\"\n</code></pre> <p>Executing the commands above will download the artifacts related to the experiment run <code>&lt;MLFLOW_RUN_UUID&gt;</code> to this repository's subdirectory  <code>models</code>. However, the specific subdirectory that is relevant for our  modules to load will be <code>./models/&lt;MLFLOW_RUN_UUID&gt;/output.txt</code>.</p> <p>Now, let's proceed and spin up an inference server using the package  that exists within the repository.</p>"},{"location":"runai/07c-deployment/#running-the-api-server","title":"Running the API Server","text":"<p>Run the FastAPI server using Gunicorn:</p> Coder Workspace TerminalUsing Run:ai <pre><code># Running in a working `project` repository\nconda activate project\nexport MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\ngunicorn project_package_fastapi.main:APP \\\n    -k uvicorn.workers.UvicornWorker \\\n    -b 0.0.0.0:8080 -w 2 -t 90 --chdir src\n</code></pre> <p>And with that, the link to our document site for our server would be given by Coder at the bottom right notification, or under the  Ports tab right beside the Terminal tab. The link should look similar to this:</p> <pre><code>export MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\nrunai submit \\\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-inference \\\n    -i registry.aisingapore.net/project-path/gpu:0.1.0 \\\n    --working-dir /home/aisg/project/src \\\n    --existing-pvc claimname=&lt;NAME_OF_DATA_SOURCE&gt;,path=/&lt;NAME_OF_DATA_SOURCE&gt; \\\n    --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \\\n    --service-type external-url,port=8080:8080 \\\n    --command -- gunicorn project_package_fastapi.main:APP \\\n        -k uvicorn.workers.UvicornWorker \\\n        -b 0.0.0.0:8080 -w 2 -t 90\n</code></pre> <p>And with that, the link to our document site for our server would be given by Run:ai. The link should look similar to this:</p> <p></p> <p>In another terminal, use the <code>curl</code> command to submit a request to the API:</p> Coder Workspace Terminal <pre><code>curl -X POST \\\n    localhost:8080/api/v1/model/predict \\\n    -H 'Content-Type: application/json' \\\n    -d '\"string\"'\n</code></pre> <p>Output sample:</p> <pre><code>{\"data\":[{\"input\":\"string\"}]}\n</code></pre> <p>With the returned JSON object, we have successfully submitted a request to the FastAPI server and it returned predictions as part of the response.</p>"},{"location":"runai/07c-deployment/#pydantic-settings","title":"Pydantic Settings","text":"<p>Now you might be wondering, how does the FastAPI server knows the path to the model for it to load? FastAPI utilises Pydantic, a library for  data and schema validation, as well as settings management. There's a  class called <code>Settings</code> under the module <code>src/project_package_fastapi/config.py</code>. This class  contains several fields: some are defined and some others not. The  <code>MODEL_UUID</code> field inherits its value from the environment variables.</p> <p><code>src/project_package_fastapi/config.py</code>: <pre><code>...\nclass Settings(pydantic_settings.BaseSettings):\n\n    API_NAME: str = \"project_package_fastapi\"\n    API_V1_STR: str = \"/api/v1\"\n    LOGGER_CONFIG_PATH: str = \"../conf/logging.yaml\"\n\n    MODEL_UUID: str\n...\n</code></pre></p> <p>FastAPI automatically generates interactive API documentation for easy viewing of all the routers/endpoints you have made available for the server. You can view the documentation through <code>&lt;API_SERVER_URL&gt;:&lt;PORT&gt;/docs</code>. </p> Reference Link(s) <ul> <li>PyTorch Tutorials - Saving and Loading Models</li> <li>FastAPI Docs</li> <li>Pydantic Docs - Settings Management</li> <li><code>curl</code> tutorial</li> </ul>"},{"location":"runai/08c-batch-inferencing/","title":"Batch Inferencing","text":"<p>Info</p> <p>The guide below is only meant for reference only and not meant to be followed verbatim. You may need to generate your own guide  site if you require guidance specifically for your own project.</p> <p>Some problem statements do not warrant the deployment of an API server but instead methods for conducting batched inferencing where a batch of data is provided to a script and it is able to churn out a set of predictions, perhaps exported to a file.</p> <p>This template provides a Python script (<code>src/batch_infer.py</code>) and a  configuration file (<code>conf/batch_infer.yaml</code>) for this purpose. </p> <p>Let's first create some sample data on our Coder workspace for us to conduct batch inferencing on:</p> Coder Workspace TerminalUsing Run:ai <pre><code>mkdir -p /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project/data/batch-infer &amp;&amp; cd $_\necho -n \"Output1\" &gt; in1.txt\necho -n \"Output2\" &gt; in2.txt\necho -n \"Output3\" &gt; in3.txt\n</code></pre> <pre><code>runai submit \\\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-download-batch-data \\\n    -i alpine \\\n    --working-dir /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project/data/batch-infer \\\n    --existing-pvc claimname=&lt;NAME_OF_DATA_SOURCE&gt;,path=/&lt;NAME_OF_DATA_SOURCE&gt; \\\n    --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \\\n    --command -- /bin/bash -c \"echo -n 'Output1' &gt; in1.txt &amp;&amp; \\\n        echo -n 'Output2' &gt; in2.txt &amp;&amp; \\\n        echo -n 'Output3' &gt; in3.txt\"\n</code></pre> <p>To execute the batch inferencing script:</p> Coder Workspace TerminalUsing Run:ai <pre><code># Navigate back to root directory\ncd \"$(git rev-parse --show-toplevel)\"\nconda activate project\npython src/batch_infer.py\n</code></pre> <pre><code>runai submit \\\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-batch-inference \\\n    -i alpine \\\n    --working-dir /home/aisg/project \\\n    --existing-pvc claimname=&lt;NAME_OF_DATA_SOURCE&gt;,path=/&lt;NAME_OF_DATA_SOURCE&gt; \\\n    --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \\\n    --command -- python src/batch_infer.py \\\n        output_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project/batch_infer_res.jsonl \\\n        input_data_dir=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/project/data/batch-infer\n</code></pre> <p>The script will log to the terminal the location of the <code>.jsonl</code> file (<code>batch-infer-res.jsonl</code>) containing predictions that look like such:</p> <pre><code>...\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in1.txt\", \"prediction\": \"Output1\"}\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in2.txt\", \"prediction\": \"Output2\"}\n{\"time\": \"2024-02-29T10:09:00+0000\", \"text_filepath\": \"./data/batch-infer/in3.txt\", \"prediction\": \"Output3\"}\n...\n</code></pre> <p>The <code>hydra.job.chdir=True</code> flag writes the <code>.jsonl</code> file containing the predictions to a subdirectory within the <code>outputs</code> folder. See  here for more information on outputs generated by Hydra.</p>"},{"location":"setting-up/01-prerequisites/","title":"Prerequisites","text":""},{"location":"setting-up/01-prerequisites/#software-tooling-prerequisites","title":"Software &amp; Tooling Prerequisites","text":"<p>Aside from an internet connection, you may need the following to follow through with the guide, depending on your project's requirements:</p> <ul> <li>PC with the following installed:<ul> <li>If your machine is with a Windows OS, use PowerShell   instead of the default Command (<code>cmd.exe</code>) shell. Best if you   resort to Windows Terminal.</li> <li>Web browser</li> <li>Terminal</li> <li>Git</li> <li>Rancher Desktop or Docker Engine:   Client-server application for containerising applications as well   as interacting with the Docker daemon.<ul> <li>For Linux users, you may install the Docker Engine (Docker    daemon) directly.</li> <li>For Windows or macOS users, the Docker daemon can be installed   through Rancher Desktop.</li> </ul> </li> <li>miniconda: for Python virtual environment management.</li> <li><code>kubectl</code>: CLI for Kubernetes.</li> <li>AWS CLI: CLI for AWS services.<ul> <li>You may choose to just use <code>boto3</code>, the Python SDK    for AWS instead, to interact with the ECS service within a    Python environment. However, this does not fall under the    scope of this guide.</li> </ul> </li> <li><code>gcloud</code> CLI: CLI for interacting with GCP services.</li> <li>(Optional) <code>helm</code>: CLI for Kubernetes' package    manager.</li> </ul> </li> <li>Access to a project on a Run:ai cluster.   See here for more information.</li> <li>Credentials for an S3-compatible service (MinIO, etc).   See here for more information.</li> <li>Credentials for a Docker registry.   See here for more information.</li> <li>Access to a project on Google Cloud Platform.   See here for more information.</li> <li>Credentials for an MLflow Tracking server.   See here for more information.</li> </ul> <p>If you're using AISG infrastructure for your project, you may also need the following:</p> <ul> <li>NUS Staff/Student account.</li> <li>Azure account provisioned by AI Singapore.</li> <li>PC with the following installed:<ul> <li>Pulse Secure<ul> <li>Refer to NUS IT eGuides for installation guides.</li> </ul> </li> <li>For the AWS CLI, you may be using it for interacting with the AI    Singapore's Elastic Cloud Storage (ECS) service through the S3 protocol.</li> </ul> </li> <li>Access to a project on AI Singapore's Run:ai cluster.  </li> <li>Credentials for AI Singapore's Elastic Cloud Storage (ECS) service. </li> <li>Credentials for AI Singapore's Harbor registry. </li> </ul> <p>Note</p> <p>If you are supposed to have access to any of AISG's infrastructure,  and you do not have any of the required credentials, please verify  with or notify the MLOps team at <code>mlops@aisingapore.org</code>.</p> <p>Info</p> <p>Wherever relevant, you can toggle between the different commands that need to be executed for either Linux/macOS or the Windows  environment (PowerShell). See below for an example:</p> Linux/macOSWindows PowerShell <pre><code># Get a list of files/folders in current directory\n$ ls -la\n</code></pre> <pre><code># Get a list of files/folders in current directory\n$ Get-ChildItem . -Force\n</code></pre> <p>Warning</p> <p>If you are on Windows OS, you would need to ensure that the  files you've cloned or written on your machine be with <code>LF</code> line endings. Otherwise, issues would arise when Docker containers  are being built or run. See here on how to configure  consistent line endings for a whole folder or workspace using  VSCode.</p>"},{"location":"setting-up/01-prerequisites/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>If you're using Rancher Desktop, you might encounter issues with    regards to the lack of CPU and memory.<ul> <li>For Mac/Linux users, from the main window, click on the gear    button on the top right.   Then, proceed to the Virtual Machines section and increase your    CPU and memory resources directly.</li> <li>For Windows users, create a <code>.wslconfig</code> file user    <code>%UserProfile%</code> with the following content:   <pre><code>[wsl2]\nmemory=8GB\n</code></pre>   Change the amount of memory to something you're comfortable with    giving up to build Docker images.</li> </ul> </li> <li>For Windows users, if you have both Rancher and Docker Desktop    installed, you may need to disable the networking tunnel:<ul> <li>From the gear button on the top right, go to the WSL section    under the Network tab. From there, uncheck the <code>Enable networking    tunnel</code>.</li> </ul> </li> </ul>"},{"location":"setting-up/02-preface/","title":"Preface","text":""},{"location":"setting-up/02-preface/#repository-setup","title":"Repository Setup","text":"<p>This repository provides an end-to-end template for AI engineers to  onboard their AI projects. Instructions for generating this template is  detailed in the <code>cookiecutter</code> template's repository's  <code>README.md</code>.</p> <p>While this repository provides users with a set of boilerplates, here  you are also presented with a linear guide on how to use them. The  boilerplates are rendered and customised when you generated this  repository using <code>cookiecutter</code>.</p> <p>Info</p> <p>You can begin by following along the guide as it brings you through a simple problem statement and once you've grasped what this template has to offer, you can deviate from it as much as you wish and customise it to your needs.</p> <p>Since we will be making use of this repository in multiple environments,  ensure that this repository is pushed to a remote. After creating the remote repository, retrieve the remote URL and push the local repository to remote:</p> <pre><code>git init\ngit remote add origin &lt;REMOTE_URL&gt;\ngit add .\ngit config user.email \"&lt;YOUR_AISG_EMAIL&gt;\"\ngit config user.name \"&lt;YOUR_NAME&gt;\"\ngit commit -m \"Initial commit.\"\ngit push -u origin main\n</code></pre> <p>For AISG projects</p> <p>You would most probably be resorting to  AI Singapore's GitLab instance as the remote. Refer  to this section on creating a blank remote repository  (or project using GitLab's terminology).  </p> <p>Go to this section for more information on  interacting with the on-premise GitLab instance.</p>"},{"location":"setting-up/02-preface/#guides-problem-statement","title":"Guide's Problem Statement","text":"<p>This guide follows the base version of the template. For other problem  templates, you may need to generate it yourself by following the steps in the README under the Usage section.</p> <p>For the other problem templates, you can check their problem statement below for reference.</p> Computer Vision <p>For this guide, we will work towards building a neural network that  is able to classify handwritten digits, widely known as the MNIST  use-case. The model is then to be deployed through a REST API and  used for batch inferencing as well. The raw dataset to be used is obtainable through a Google Cloud  Storage bucket; instructions for downloading the data into your  development environment are detailed under \"Data Storage &amp;  Versioning\" in their respective sections.</p> <p>Info</p> <p>License: Yann LeCun and Corinna Cortes hold the copyright  of MNIST dataset. MNIST dataset is made available under the  terms of the Creative Commons Attribution-Share Alike 3.0 license.</p>"}]}